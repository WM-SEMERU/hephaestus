# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/07_Experiment.ipynb (unless otherwise specified).

__all__ = ['HephaestusModelEvaluation', 'plotTrainingAccuracies', 'plotPerfectPredictionAccuracies',
           'plotAllPerfectPredictionAccuracies', 'plotFailedPredictionRates']

# Cell
#export
from typing import Dict, Any, List
import pandas as pd

import sys
sys.path.append("..")

from .AbstractMethod import *
from .IOUtils import *
from .DatasetConstruction import *
from .HephaestusModel import *
from .TrainModels import *

# Cell
class HephaestusModelEvaluation:
    """
    Helper class to centralize computations when evaluating the effectiveness of a `HephaestusModel`.

    Required Arguments:

    - `model`: The `HephaestusModel` to evaluate
    - `testSourceMethods`: A list of AbstractMethods representing the buggy abstract methods in the testing set. These
      methods should not appear at all in the training or validation sets that were used to train the model.
    - `testTargetMethods`: A list of AbstractMethods representing the fixed abstract methods in the testing set. These
      methods should not appear at all in the training or validation sets that were used to train the model.

    Optional Arguments:

    - `isControl`: Set to `True` if the model represents the control, i.e. the model was trained purely with
      AbstractMethods and not with EditOperations. Defaults to `False`.

    Once created, the `HephaestusModelEvaluation` will contain the following attributes which can be freely accessed:

    - `outputMethods`: A list of AbstractMethods which were translated from the given buggy (source) AbstractMethods.
      These represent the model's predictions of what it thinks the fixed methods are. Some of these entries may be
      `None`, representing failed predictions.
    - `numPerfectPredictions`: The number of `outputMethods` whose tokens exactly match the actual fixed method according
      to the provided `testTargetMethods`.
    - `perfectPredictionRatio`: The ratio of perfect predictions to total predictions, in the range [0, 1].
    - `numFailedPredictions`: The number of `outputMethods` which were not able to be predicted due to ill-formed
      EditOperations or other factors. These are represented by `None` values in `outputMethods`.
    - `failedPredictionRatio`: The ratio of failed predictions to total predictions, in the range [0, 1].
    - `trainingStats`: A Pandas DataFrame representing model information while it was training, as in
      `HephaestusModel.getTrainingStats`.
    """

    def __init__(self,
        model: HephaestusModel,
        testSourceMethods: List[AbstractMethod],
        testTargetMethods: List[AbstractMethod],
        isControl: bool = False
    ) -> None:

        # use the model to determine output methods
        self.outputMethods = model.translate(testSourceMethods, applyEditOperations = not isControl)

        # sanity check -- make sure that the lengths of all lists of methods are equal
        if not len(testSourceMethods) == len(testTargetMethods) == len(self.outputMethods):
            raise RuntimeError("HephaestusModelEvaluation: method number mismatch")

        # determine number of perfect predictions and failed predictions
        self.numPerfectPredictions = 0
        self.numFailedPredictions = 0
        for outputMethod, targetMethod in zip(self.outputMethods, testTargetMethods):
            if outputMethod == targetMethod:
                self.numPerfectPredictions += 1
            if outputMethod is None:
                self.numFailedPredictions += 1

        # obtain perfect prediction and failed prediction ratios
        self.perfectPredictionRatio = self.numPerfectPredictions / len(self.outputMethods)
        self.failedPredictionRatio = self.numFailedPredictions / len(self.outputMethods)

        # get the training statistics
        self.trainingStats = model.getTrainingStats()

# Cell
def plotTrainingAccuracies(
    evaluations: List[HephaestusModelEvaluation],
    lineLabels: List[str],
    title: str
) -> None:
    """
    Plots the training accuracies of the given `evaluations` as a line graph per training step. The line labels are the
    given `lineLabels`. The title is the given `title`.
    """

    # create dataframe
    data = {}
    for evaluation, label in zip(evaluations, lineLabels):
        data[label] = evaluation.trainingStats["trainAccuracy"].to_list()
    frame = pd.DataFrame(data, index = evaluations[0].trainingStats["step"].to_list())

    # create plot
    plot = frame.plot.line(
        title = title,
        xlabel = "Training Step",
        ylabel = "Training Accuracy %",
        grid = True
    )

# Cell
def plotPerfectPredictionAccuracies(
    evaluations: List[HephaestusModelEvaluation],
    xLabels: List[str],
    title: str
) -> pd.DataFrame:
    """
    Plots the perfect prediction accuracies of the given `evaluations` as a bar graph. The x-axis labels are the
    given `xlabels`. The title of the graph is the given `title`. Returns the dataframe used to create the graph.
    """

    # create dataframe
    frame = pd.DataFrame({
        "model": xLabels,
        "accuracy": [100 * e.perfectPredictionRatio for e in evaluations]
    })

    # create plot
    plot = frame.plot.bar(
        x = "model",
        y = "accuracy",
        ylim = (0, max(frame["accuracy"]) + 2),
        xlabel = "Model",
        ylabel = "Perfect Prediction Accuracy %",
        title = title,
        rot = 0
    )

    # annotate the bars with their values
    for p in plot.patches:
        plot.annotate(
            "{:.1f}".format(p.get_height()),
            (p.get_x() + p.get_width() / 2, p.get_height() + 0.5),
            ha = "center",
            size = 10
        )

    return frame

# Cell
def plotAllPerfectPredictionAccuracies(
    evaluations: Dict[str, HephaestusModelEvaluation],
    xLabels: List[str]
) -> None:
    """
    Plots the perfect predictions accuracies of all given `evaluations` as a nested bar graph. The x-axis labels are the
    given `xLabels`. Returns the dataframe used to create the graph.
    """

    # create dataframe
    data = {}
    for key in evaluations:
        data[key] = [100 * e.perfectPredictionRatio for e in evaluations[key]]
    frame = pd.DataFrame(data, index = xLabels)

    # create plot
    plot = frame.plot.bar(
        ylim = (0, frame.max().max() + 2),
        xlabel = "Model",
        ylabel = "Perfect Prediction Accuracy %",
        title = "Perfect Prediction Accuracies of All Models",
        rot = 0
    )

    # annotate plot
    plot.legend(title = "Training Parameters")
    for p in plot.patches:
        plot.annotate(
            "{:.1f}".format(p.get_height()),
            (p.get_x() + p.get_width() / 2, p.get_height() + 0.5),
            ha = "center",
            size = 8
        )

    return frame

# Cell
def plotFailedPredictionRates(
    evaluations: List[HephaestusModelEvaluation],
    xLabels: List[str],
    title: str
) -> pd.DataFrame:
    """
    Plots the failed prediction rates of the given `evaluations` as a bar graph. The x-axis labels are the
    given `xlabels`. The title of the graph is the given `title`. Returns the dataframe used to create the graph.
    """

    # create dataframe
    frame = pd.DataFrame({
        "model": xLabels,
        "rate": [100 * e.failedPredictionRatio for e in evaluations]
    })

    # create plot
    plot = frame.plot.bar(
        x = "model",
        y = "rate",
        ylim = (0, max(frame["rate"]) + 0.5),
        xlabel = "Model",
        ylabel = "Failed Prediction Rate %",
        title = title,
        rot = 0
    )

    # annotate the bars with their values
    for p in plot.patches:
        plot.annotate(
            "{:.1f}".format(p.get_height()),
            (p.get_x() + p.get_width() / 2, p.get_height() + 0.03),
            ha = "center",
            size = 10
        )

    return frame