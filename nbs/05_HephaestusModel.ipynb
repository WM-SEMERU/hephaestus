{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp HephaestusModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#hide\n",
    "from typing import Union, List, Optional, Tuple\n",
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from hephaestus.EditOperations import *\n",
    "from hephaestus.CondenseEditOperations import *\n",
    "from hephaestus.AbstractMethod import *\n",
    "from hephaestus.IOUtils import *\n",
    "from hephaestus.DatasetConstruction import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#If you need to install any dependencies, see below:\n",
    "#%pip install --upgrade OpenNMT-py==2.0.1\n",
    "#%pip install nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HephaestusModel\n",
    "\n",
    "> Encapsulates NMT operations on AbstractMethods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HephaestusModel:\n",
    "    \"\"\"\n",
    "    The `HephaestusModel` is the means through which buggy AbstractMethods are translated into fixed ones. Each\n",
    "    `HephaestusModel` occupies a directory which contains stored models, vocabularies, and configuration files.\n",
    "\n",
    "    Required args:\n",
    "    - `modelDir`: The directory which stores files pertaining to the model. You can use a directory which already\n",
    "      contains the necessary files (previously generated from a different `HephaestusModel`), in which case the\n",
    "      model will not have to be trained again. If you provide a directory that does not exist, the `HephaestsuModel`\n",
    "      will attempt to create it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, modelDir: str) -> None:\n",
    "\n",
    "        # set up constants\n",
    "        self.__MODEL_DIR =         modelDir if os.path.sep == \"/\" else modelDir.replace(\"/\", os.path.sep)\n",
    "        self.__CONFIG_PATH =       os.path.join(self.__MODEL_DIR, \"config.yaml\")\n",
    "        self.__SAVE_DATA_PATH =    os.path.join(self.__MODEL_DIR, \"save_data\")\n",
    "        self.__SOURCE_VOCAB_PATH = os.path.join(self.__MODEL_DIR, \"save_data.vocab.src\")\n",
    "        self.__TARGET_VOCAB_PATH = os.path.join(self.__MODEL_DIR, \"save_data.vocab.tgt\")\n",
    "        self.__TRAIN_OUTPUT_PATH = os.path.join(self.__MODEL_DIR, \"train_output.txt\")\n",
    "        self.__RAW_OUTPUT_PATH =   os.path.join(self.__MODEL_DIR, \"raw_output.txt\")\n",
    "        self.__POST_OUTPUT_PATH =  os.path.join(self.__MODEL_DIR, \"postprocessed_output.txt\")\n",
    "        self.__SAVE_MODEL_PREFIX = \"model\"\n",
    "        self.__SAVE_MODEL_PATH =   os.path.join(self.__MODEL_DIR, self.__SAVE_MODEL_PREFIX)\n",
    "        self.__FINAL_MODEL_PATH =  os.path.join(self.__MODEL_DIR, self.__SAVE_MODEL_PREFIX + \"_final.pt\")\n",
    "\n",
    "        # create the modelDir directory if it doesn't already exist\n",
    "        if not os.path.isdir(self.__MODEL_DIR):\n",
    "            os.makedirs(self.__MODEL_DIR)\n",
    "\n",
    "    def train(self,\n",
    "\n",
    "        trainSource: str,\n",
    "        trainTarget: str,\n",
    "        validSource: str,\n",
    "        validTarget: str,\n",
    "\n",
    "        numCheckpoints: int = 10,\n",
    "        numGPUs: int = 1,\n",
    "\n",
    "        embeddingSize: int = 512,\n",
    "\n",
    "        rnnType: str = \"LSTM\",\n",
    "        rnnSize: int = 256,\n",
    "        numLayers: int = 2,\n",
    "\n",
    "        numTrainingSteps: int = 50000,\n",
    "        numValidations: int = 10,\n",
    "        dropout: int = 0.2\n",
    "\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Trains the model with the given parameters. Files containing AbstractMethods should have one per line with\n",
    "        tokens separated by spaces. 'source' files must contain AbstractMethods. 'target' files may contain\n",
    "        AbstractMethods or CompoundOperations in machine string format.\n",
    "\n",
    "        As the training progesses, checkpoint model files are created which follow the format `model_step_#.pt`, where\n",
    "        `#` corresponds to the training step number. Once training is complete, the finalized model is outputted to\n",
    "        `model_final.pt`. In addition, training command output is written to `train_output.txt`.\n",
    "\n",
    "        Default parameter values are such that they resemble the most successful NMT model in\n",
    "        [this paper](https://arxiv.org/pdf/1812.08693.pdf) as closely as possible.\n",
    "\n",
    "        Parameters:\n",
    "        - Data and vocabulary:\n",
    "            - `trainSource`: Required. File name containing training source data. Must be buggy AbstractMethods.\n",
    "            - `trainTarget`: Required. File name containing training target data. Can be either non-buggy\n",
    "              AbstractMethods or CompoundOperations in machine string format.\n",
    "            - `validSource`: Required. File name containing validation source data. Must be buggy AbstractMethods.\n",
    "            - `validTarget`: Required. File name containing validation target data. Must be the same type of data which\n",
    "              is contained in the file denoted by `trainTarget`.\n",
    "        - General options:\n",
    "            - `numCheckpoints`: Number of times a checkpoint model is saved; e.g. if `numTrainingSteps` is 50,000 and\n",
    "              `numCheckpoints` is 10, then a checkpoint will be saved after every 5,000 training steps. Defaults to 10.\n",
    "            - `numGPUs`: Number of GPUs to use concurrently during training. If set to 0, then the CPU is used. Defaults\n",
    "              to 1.\n",
    "        - Model options:\n",
    "            - `embeddingSize`: Word embedding size for source and target. Defaults to 512.\n",
    "        - Encoder/decoder options:\n",
    "            - `rnnType`: Gate type to use in RNN encoder and decoder. Can be `\"LSTM\"` or `\"GRU\"`. Defaults to `\"LSTM\"`.\n",
    "            - `rnnSize`: Size of encoder and decoder RNN hidden states. Defaults to 256.\n",
    "            - `numLayers`: Number of layers each in the encoder and decoder. Defaults to 2.\n",
    "        - Learning options:\n",
    "            - `numTrainingSteps`: Number of training steps to perform. Defaults to 50,000.\n",
    "            - `numValidations`: `validSteps`: Number of validations to perform during training; e.g. if `numTrainingSteps`\n",
    "              is 50,000 and `numValidations` is 10, then validation will occur after every 5,000 training steps. Defaults\n",
    "              to 10.\n",
    "            - `dropout`: Dropout probability. Defaults to 0.2.\n",
    "        \"\"\"\n",
    "\n",
    "        # write config file using same parameters as passed to this method, note that locals() contains self\n",
    "        HephaestusModel.__writeConfigFile(**locals())\n",
    "        \n",
    "        # build vocabulary, first create empty files for the vocab\n",
    "        for filename in (self.__SOURCE_VOCAB_PATH, self.__TARGET_VOCAB_PATH):\n",
    "            open(filename, \"w\").close()\n",
    "        runCommand('onmt_build_vocab -config \"{}\" -n_sample -1'.format(self.__CONFIG_PATH))\n",
    "\n",
    "        # delete previous model files\n",
    "        for file in os.listdir(self.__MODEL_DIR):\n",
    "            if re.search(r\"^\" + self.__SAVE_MODEL_PREFIX + r\"_(?:step_[0-9]+|final).pt$\", file):\n",
    "                os.remove(os.path.join(self.__MODEL_DIR, file))\n",
    "    \n",
    "        # train the model and write output to the appropriate file\n",
    "        trainOutput = runCommand('onmt_train -config \"{}\"'.format(self.__CONFIG_PATH))\n",
    "        with open(self.__TRAIN_OUTPUT_PATH, \"w\") as f:\n",
    "            f.write(trainOutput)\n",
    "\n",
    "        # find and release the highest trained model\n",
    "        latestModel = None\n",
    "        maxNum = 0\n",
    "        for file in os.listdir(self.__MODEL_DIR):\n",
    "            match = re.search(r\"^\" + self.__SAVE_MODEL_PREFIX + r\"_(?:step_([0-9]+)|final).pt$\", file)\n",
    "            if match:\n",
    "                stepNum = int(match.group(1))\n",
    "                if stepNum > maxNum:\n",
    "                    latestModel = os.path.join(self.__MODEL_DIR, file)\n",
    "                    maxNum = stepNum\n",
    "\n",
    "        if latestModel is not None:\n",
    "            runCommand('onmt_release_model --model \"{}\" --output \"{}\"'.format(latestModel, self.__FINAL_MODEL_PATH))\n",
    "    \n",
    "    def getTrainingStats(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns a pandas dataframe describing training statistics; the dataframe has the following columns:\n",
    "        \n",
    "        - `step`: The training step in increments of 50\n",
    "        - `trainAccuracy`: Model accuracy with respect to the **training** set\n",
    "        - `validAccuracy`: Validation accuracy. These values will likely not be present for every row.\n",
    "        - `crossEntropy`: Cross-entropy value\n",
    "        \"\"\"\n",
    "        \n",
    "        # create empty dataframe and initialize trainStep value\n",
    "        frame = pd.DataFrame(columns = [\"step\", \"trainAccuracy\", \"validAccuracy\", \"crossEntropy\"])\n",
    "        trainStep = -1\n",
    "\n",
    "        # read through lines of the training output file\n",
    "        with open(self.__TRAIN_OUTPUT_PATH, \"r\") as f:\n",
    "\n",
    "            for line in f:\n",
    "                \n",
    "                line = line.strip()\n",
    "                \n",
    "                # attempt to match against a line that has training accuracy and the like\n",
    "                match = re.search(r\"^\\[[^\\]]*INFO\\] *Step *(\\d+)/ *\\d+; *acc: *(.+?);.+?xent: *(.+?);\", line)\n",
    "                if match:\n",
    "\n",
    "                    trainStep = int(match.group(1))\n",
    "                    accuracy = float(match.group(2))\n",
    "                    xEntropy = float(match.group(3))\n",
    "\n",
    "                    frame.loc[len(frame)] = [trainStep, accuracy, None, xEntropy]\n",
    "\n",
    "                # attmpt to match against a line that has validation accuracy info\n",
    "                match = re.search(r\"^\\[[^\\]]*INFO\\] *Validation accuracy: *((?:\\d+\\.)?\\d+)\", line)\n",
    "                if match and trainStep > 0:\n",
    "                    frame.at[len(frame) - 1, \"validAccuracy\"] = float(match.group(1))\n",
    "\n",
    "        # set the type of the \"step\" column to int, then return the frame\n",
    "        frame[\"step\"] = frame[\"step\"].astype(int)\n",
    "        return frame\n",
    "\n",
    "    def translate(self,\n",
    "        buggy: Union[str, AbstractMethod, List[AbstractMethod]],\n",
    "        modelFile: str = None,\n",
    "        applyEditOperations: bool = True\n",
    "    ) -> Union[Optional[AbstractMethod], List[Optional[AbstractMethod]]]:\n",
    "        \"\"\"\n",
    "        Translates the given `buggy` AbstractMethods into supposedly fixed AbstractMethods, writes them to\n",
    "        `<model_directory>/postprocessed_output.txt`, and then returns them. The raw output of the model is written\n",
    "        to `<model_directory>/raw_output.txt` in case you want to access that as well. Depending on what type of\n",
    "        value is passed to `buggy`, the return value of this method changes according to the following:\n",
    "\n",
    "        | `buggy` type           | Return type                      |\n",
    "        | :--------------------- | :------------------------------- |\n",
    "        | `str` (a file)         | `List[Optional[AbstractMethod]]` |\n",
    "        | `AbstractMethod`       | `Optional[AbstractMethod]`       |\n",
    "        | `List[AbstractMethod]` | `List[Optional[AbstractMethod]]` |\n",
    "\n",
    "        A `None` return value means that the model was unable to translate that abstract method correctly. This\n",
    "        could be due to the model outputting non well-formed CompoundOperations, among other things. These will\n",
    "        appear as blank lines in `postprocessed_output.txt`.\n",
    "\n",
    "        Optional args:\n",
    "        - `modelFile`: A `.pt` file which is used for translation instead of the default `model_final.pt`\n",
    "        - `applyEditOperations`: When set to True, the model output is interpreted as CompoundOperations and a\n",
    "          postprocessing stage occurs where the outputted CompoundOperations are applied to the inputted\n",
    "          AbstractMethods. When set to False, the raw output is interpreted as AbstractMethods and returned\n",
    "          without a postprocessing stage; in this case, the contents of `raw_output.txt` and\n",
    "          `postprocessed_output.txt` are identical. If the model was trained with EditOperations,\n",
    "          `applyEditOperations` should be True; if the model was trained with just AbstractMethods as in for\n",
    "          the control group, then this should be False. Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        # determine which model file to use, and raise an error if it doesn't exist\n",
    "        if modelFile is None:\n",
    "            modelFile = self.__FINAL_MODEL_PATH\n",
    "        if not os.path.isfile(modelFile):\n",
    "            raise FileNotFoundError(\"HephaestusModel: model not found -- {}\".format(modelFile))\n",
    "        \n",
    "        # write the AbstractMethods to a file if they were given directly\n",
    "        buggyFile = None\n",
    "        if type(buggy) in (AbstractMethod, list):\n",
    "            buggyFile = os.path.join(self.__MODEL_DIR, \"input.txt\")\n",
    "            writeAbstractMethodsToFile(buggyFile, buggy if type(buggy) is list else [buggy])\n",
    "        else:\n",
    "            buggyFile = buggy\n",
    "        \n",
    "        # translate the buggy methods\n",
    "        command = 'onmt_translate -model \"{}\" -src \"{}\" -output \"{}\"'.format(modelFile, buggyFile, self.__RAW_OUTPUT_PATH)\n",
    "        if getYamlParameter(self.__CONFIG_PATH, \"world_size\") is not None: # if GPU should be used\n",
    "            command += \" --gpu 0\"\n",
    "        runCommand(command)\n",
    "\n",
    "        # strip the last line of the output file because OpenNMT likes to put a newline at the end\n",
    "        with open(self.__RAW_OUTPUT_PATH, \"r+\") as outputFile:\n",
    "            lines = outputFile.readlines()\n",
    "            lines[-1] = lines[-1].strip()\n",
    "            outputFile.seek(0)\n",
    "            outputFile.writelines(lines)\n",
    "            outputFile.truncate()\n",
    "\n",
    "        # get all inputted AbstractMethods\n",
    "        inputMethods = []\n",
    "        if type(buggy) in (AbstractMethod, list):\n",
    "            inputMethods = buggy if type(buggy) is list else [buggy]\n",
    "        else:\n",
    "            inputMethods = readAbstractMethodsFromFile(buggyFile)\n",
    "        \n",
    "        # If edit ops should be applied, then extract the operations from the output file and attempt to\n",
    "        # apply them to the input methods. Assign a None value to a fixed method if its corresponding\n",
    "        # operations were not able to be read, or if the operations are illegal (i.e. modifies out of bounds\n",
    "        # tokens). Copy input methods before applying edit operations so that the original remains unmodified.\n",
    "        fixedMethods = []\n",
    "        if applyEditOperations:\n",
    "            operations = readCompoundOperationsFromFile(self.__RAW_OUTPUT_PATH)\n",
    "            for inputMethod, opList in zip(inputMethods, operations):\n",
    "                if opList is None:\n",
    "                    fixedMethods.append(None)\n",
    "                else:\n",
    "                    try:\n",
    "                        inputMethodCopy = deepcopy(inputMethod)\n",
    "                        inputMethodCopy.applyEditOperations(opList)\n",
    "                        fixedMethods.append(inputMethodCopy)\n",
    "                    except IndexError as e:\n",
    "                        fixedMethods.append(None)\n",
    "        \n",
    "        # Simply interpret the output as abstract methods if not interpreting as edit operations\n",
    "        else:\n",
    "            fixedMethods = readAbstractMethodsFromFile(self.__RAW_OUTPUT_PATH)\n",
    "        \n",
    "        # Make sure the number of fixed methods equals the number of inputted methods -- this can differ if the\n",
    "        # model fails to translate one of the inputs.\n",
    "        numFails = len(inputMethods) - len(fixedMethods)\n",
    "        if numFails > 0:\n",
    "            raise RuntimeError(\"HephaestusModel: failed to translate {} input(s)\".format(numFails))\n",
    "        \n",
    "        # write the fixed methods to the postprocessed output file, substituting null methods with blank lines\n",
    "        writeAbstractMethodsToFile(\n",
    "            self.__POST_OUTPUT_PATH,\n",
    "            [\" \" if method is None else method for method in fixedMethods]\n",
    "        )\n",
    "\n",
    "        # return fixed methods\n",
    "        return fixedMethods if type(buggy) is list else fixedMethods[0]\n",
    "\n",
    "    def __writeConfigFile(self, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Creates the config file. Takes the same arguments as `HephaestusModel.train`.\n",
    "        \"\"\"\n",
    "\n",
    "        def makeHeader(label: str) -> str:\n",
    "            \"\"\"Makes a nice header for the config file sections.\"\"\"\n",
    "            return \"#\" * 80 + \"\\n# {0:<77}#\\n\".format(label) + \"#\" * 80\n",
    "        \n",
    "        # calculate some parameters which are defined differently in OpenNMT\n",
    "        saveCheckpointSteps = max(kwargs[\"numTrainingSteps\"] // kwargs[\"numCheckpoints\"], 1)\n",
    "        validSteps = max(kwargs[\"numTrainingSteps\"] // kwargs[\"numValidations\"], 1)\n",
    "        \n",
    "        lines = [\n",
    "            '# AUTOGENERATED',\n",
    "            '',\n",
    "            makeHeader(\"GENERAL OPTIONS\"),\n",
    "            '',\n",
    "            '# Base path for objects that will be saved, e.g. vocab, embeddings, etc.',\n",
    "            'save_data: \"{}\"'.format(self.__SAVE_DATA_PATH.replace(os.path.sep, \"/\")),\n",
    "            '',\n",
    "            '# Base path for saved model checkpoints',\n",
    "            'save_model: \"{}\"'.format(self.__SAVE_MODEL_PATH.replace(os.path.sep, \"/\")),\n",
    "            '',\n",
    "            '# Save a model checkpoint after X number of training steps',\n",
    "            'save_checkpoint_steps: {}'.format(saveCheckpointSteps),\n",
    "            '',\n",
    "            '# Allow overwriting existing files in the model directory',\n",
    "            'overwrite: true',\n",
    "            '',\n",
    "            makeHeader(\"VOCABULARY AND DATA\"),\n",
    "            '',\n",
    "            '# Vocabularies will be written to these files',\n",
    "            'src_vocab: \"{}\"'.format(self.__SOURCE_VOCAB_PATH.replace(os.path.sep, \"/\")),\n",
    "            'tgt_vocab: \"{}\"'.format(self.__TARGET_VOCAB_PATH.replace(os.path.sep, \"/\")),\n",
    "            '',\n",
    "            '# Defines training and validation datasets. Data is already in the correct',\n",
    "            '# format, so no need for transforms.',\n",
    "            'data:',\n",
    "            '    corpus_1:',\n",
    "            '        path_src: \"{}\"'.format(kwargs[\"trainSource\"].replace(os.path.sep, \"/\")),\n",
    "            '        path_tgt: \"{}\"'.format(kwargs[\"trainTarget\"].replace(os.path.sep, \"/\")),\n",
    "            '        transforms: []',\n",
    "            '        weight: 1',\n",
    "            '    valid:',\n",
    "            '        path_src: \"{}\"'.format(kwargs[\"validSource\"].replace(os.path.sep, \"/\")),\n",
    "            '        path_tgt: \"{}\"'.format(kwargs[\"validTarget\"].replace(os.path.sep, \"/\")),\n",
    "            '        transforms: []',\n",
    "            '',\n",
    "            makeHeader(\"MODEL\"),\n",
    "            '',\n",
    "            '# Overall type of model, here we use seq2seq',\n",
    "            'model_task: seq2seq',\n",
    "            '',\n",
    "            '# Attention method to use in encoder and decoder, mlp means Bahdanau',\n",
    "            'global_attention: mlp',\n",
    "            '',\n",
    "            '# Do not use an additional layer between the encoder and decoder',\n",
    "            'bridge: false',\n",
    "            '',\n",
    "            '# Word embedding size for source and target',\n",
    "            'word_vec_size: {}'.format(kwargs[\"embeddingSize\"]),\n",
    "            '',\n",
    "            makeHeader(\"ENCODER / DECODER\"),\n",
    "            '',\n",
    "            '# Gate type to use in RNN encoder and decoder',\n",
    "            'rnn_type: {}'.format(kwargs[\"rnnType\"]),\n",
    "            '',\n",
    "            '# Encoder and decoder are always RNNs',\n",
    "            'encoder_type: rnn',\n",
    "            'decoder_type: rnn',\n",
    "            '',\n",
    "            '# Size of encoder and decoder RNN hidden states',\n",
    "            'rnn_size: {}'.format(kwargs[\"rnnSize\"]),\n",
    "            '',\n",
    "            '# Number of layers in each the encoder and decoder',\n",
    "            'layers: {}'.format(kwargs[\"numLayers\"]),\n",
    "            '',\n",
    "            makeHeader(\"LEARNING AND OPTIMIZATION\"),\n",
    "            '',\n",
    "            '# Number of training steps to perform',\n",
    "            'train_steps: {}'.format(kwargs[\"numTrainingSteps\"]),\n",
    "            '',\n",
    "            '# Perform validation every X number of training steps',\n",
    "            'valid_steps: {}'.format(validSteps),\n",
    "            '',\n",
    "            '# Dropout probability',\n",
    "            'dropout: {}'.format(kwargs[\"dropout\"]),\n",
    "            '',\n",
    "            '# Use the Adam optimization method',\n",
    "            'optim: adam',\n",
    "            '',\n",
    "            '# Starting learning rate -- Tufano et al. use 0.0001',\n",
    "            'learning_rate: 0.0001',\n",
    "            ''\n",
    "        ]\n",
    "\n",
    "        numGPUs = kwargs[\"numGPUs\"]\n",
    "        gpuLines = []\n",
    "        if numGPUs > 0:\n",
    "            gpuLines += [\n",
    "                '# Train using {} GPU{}'.format(numGPUs, \"s\" if numGPUs > 1 else \"\"),\n",
    "                'world_size: {}'.format(numGPUs),\n",
    "                'gpu_ranks:',\n",
    "                *[\"- {}\".format(i) for i in range(numGPUs)],\n",
    "                ''\n",
    "            ]\n",
    "        else:\n",
    "            gpuLines += [\"# Train using the CPU, so no world_size parameter is provided\", \"\"]\n",
    "        \n",
    "        lines = lines[:16] + gpuLines + lines[16:]\n",
    "\n",
    "        with open(self.__CONFIG_PATH, \"w\") as file:\n",
    "            file.write(\"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"HephaestusModel.train\" class=\"doc_header\"><code>HephaestusModel.train</code><a href=\"__main__.py#L33\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>HephaestusModel.train</code>(**`trainSource`**:`str`, **`trainTarget`**:`str`, **`validSource`**:`str`, **`validTarget`**:`str`, **`numCheckpoints`**:`int`=*`10`*, **`numGPUs`**:`int`=*`1`*, **`embeddingSize`**:`int`=*`512`*, **`rnnType`**:`str`=*`'LSTM'`*, **`rnnSize`**:`int`=*`256`*, **`numLayers`**:`int`=*`2`*, **`numTrainingSteps`**:`int`=*`50000`*, **`numValidations`**:`int`=*`10`*, **`dropout`**:`int`=*`0.2`*)\n",
       "\n",
       "Trains the model with the given parameters. Files containing AbstractMethods should have one per line with\n",
       "tokens separated by spaces. 'source' files must contain AbstractMethods. 'target' files may contain\n",
       "AbstractMethods or CompoundOperations in machine string format.\n",
       "\n",
       "As the training progesses, checkpoint model files are created which follow the format `model_step_#.pt`, where\n",
       "`#` corresponds to the training step number. Once training is complete, the finalized model is outputted to\n",
       "`model_final.pt`. In addition, training command output is written to `train_output.txt`.\n",
       "\n",
       "Default parameter values are such that they resemble the most successful NMT model in\n",
       "[this paper](https://arxiv.org/pdf/1812.08693.pdf) as closely as possible.\n",
       "\n",
       "Parameters:\n",
       "- Data and vocabulary:\n",
       "    - `trainSource`: Required. File name containing training source data. Must be buggy AbstractMethods.\n",
       "    - `trainTarget`: Required. File name containing training target data. Can be either non-buggy\n",
       "      AbstractMethods or CompoundOperations in machine string format.\n",
       "    - `validSource`: Required. File name containing validation source data. Must be buggy AbstractMethods.\n",
       "    - `validTarget`: Required. File name containing validation target data. Must be the same type of data which\n",
       "      is contained in the file denoted by `trainTarget`.\n",
       "- General options:\n",
       "    - `numCheckpoints`: Number of times a checkpoint model is saved; e.g. if `numTrainingSteps` is 50,000 and\n",
       "      `numCheckpoints` is 10, then a checkpoint will be saved after every 5,000 training steps. Defaults to 10.\n",
       "    - `numGPUs`: Number of GPUs to use concurrently during training. If set to 0, then the CPU is used. Defaults\n",
       "      to 1.\n",
       "- Model options:\n",
       "    - `embeddingSize`: Word embedding size for source and target. Defaults to 512.\n",
       "- Encoder/decoder options:\n",
       "    - `rnnType`: Gate type to use in RNN encoder and decoder. Can be `\"LSTM\"` or `\"GRU\"`. Defaults to `\"LSTM\"`.\n",
       "    - `rnnSize`: Size of encoder and decoder RNN hidden states. Defaults to 256.\n",
       "    - `numLayers`: Number of layers each in the encoder and decoder. Defaults to 2.\n",
       "- Learning options:\n",
       "    - `numTrainingSteps`: Number of training steps to perform. Defaults to 50,000.\n",
       "    - `numValidations`: `validSteps`: Number of validations to perform during training; e.g. if `numTrainingSteps`\n",
       "      is 50,000 and `numValidations` is 10, then validation will occur after every 5,000 training steps. Defaults\n",
       "      to 10.\n",
       "    - `dropout`: Dropout probability. Defaults to 0.2."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(HephaestusModel.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"HephaestusModel.getTrainingStats\" class=\"doc_header\"><code>HephaestusModel.getTrainingStats</code><a href=\"__main__.py#L125\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>HephaestusModel.getTrainingStats</code>()\n",
       "\n",
       "Returns a pandas dataframe describing training statistics; the dataframe has the following columns:\n",
       "\n",
       "- `step`: The training step in increments of 50\n",
       "- `trainAccuracy`: Model accuracy with respect to the **training** set\n",
       "- `validAccuracy`: Validation accuracy. These values will likely not be present for every row.\n",
       "- `crossEntropy`: Cross-entropy value"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(HephaestusModel.getTrainingStats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"HephaestusModel.translate\" class=\"doc_header\"><code>HephaestusModel.translate</code><a href=\"__main__.py#L165\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>HephaestusModel.translate</code>(**`buggy`**:`Union`\\[`str`, [`AbstractMethod`](/hephaestus/AbstractMethod.html), `List`\\[[`AbstractMethod`](/hephaestus/AbstractMethod.html)\\]\\], **`modelFile`**:`str`=*`None`*, **`applyEditOperations`**:`bool`=*`True`*)\n",
       "\n",
       "Translates the given `buggy` AbstractMethods into supposedly fixed AbstractMethods, writes them to\n",
       "`<model_directory>/postprocessed_output.txt`, and then returns them. The raw output of the model is written\n",
       "to `<model_directory>/raw_output.txt` in case you want to access that as well. Depending on what type of\n",
       "value is passed to `buggy`, the return value of this method changes according to the following:\n",
       "\n",
       "| `buggy` type           | Return type                      |\n",
       "| :--------------------- | :------------------------------- |\n",
       "| `str` (a file)         | `List[Optional[AbstractMethod]]` |\n",
       "| [`AbstractMethod`](/hephaestus/AbstractMethod.html)       | `Optional[AbstractMethod]`       |\n",
       "| `List[AbstractMethod]` | `List[Optional[AbstractMethod]]` |\n",
       "\n",
       "A `None` return value means that the model was unable to translate that abstract method correctly. This\n",
       "could be due to the model outputting non well-formed CompoundOperations, among other things. These will\n",
       "appear as blank lines in `postprocessed_output.txt`.\n",
       "\n",
       "Optional args:\n",
       "- `modelFile`: A `.pt` file which is used for translation instead of the default `model_final.pt`\n",
       "- `applyEditOperations`: When set to True, the model output is interpreted as CompoundOperations and a\n",
       "  postprocessing stage occurs where the outputted CompoundOperations are applied to the inputted\n",
       "  AbstractMethods. When set to False, the raw output is interpreted as AbstractMethods and returned\n",
       "  without a postprocessing stage; in this case, the contents of `raw_output.txt` and\n",
       "  `postprocessed_output.txt` are identical. If the model was trained with EditOperations,\n",
       "  `applyEditOperations` should be True; if the model was trained with just AbstractMethods as in for\n",
       "  the control group, then this should be False. Defaults to True."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(HephaestusModel.translate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Let's create a small test model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HephaestusModel(\"test_model_loose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there is a directory called `test_model_loose` which will be populated with files once the model is trained. We will train the model with the loosely condensed edit operations dataset. Variables such as `DATA_SMALL_METHODS_TRAIN_BUGGY` describe the path to data files, and are defined in the `DatasetContstruction` module. Since this is just an example, a *very* small number of training steps will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-04-23 00:41:58,835 INFO] Counter vocab from -1 samples.\n",
      "[2021-04-23 00:41:58,835 INFO] n_sample=-1: Build vocab on full datasets.\n",
      "[2021-04-23 00:41:58,839 INFO] corpus_1's transforms: TransformPipe()\n",
      "[2021-04-23 00:41:58,839 INFO] Loading ParallelCorpus(../data/small/abstract_methods/train_buggy.txt, ../data/small/edit_ops/loose/train.txt, align=None)...\n",
      "[2021-04-23 00:41:59,441 INFO] Counters src:429\n",
      "[2021-04-23 00:41:59,441 INFO] Counters tgt:444\n",
      "[2021-04-23 00:41:59,441 WARNING] path test_model_loose/save_data.vocab.src exists, may overwrite...\n",
      "[2021-04-23 00:41:59,443 WARNING] path test_model_loose/save_data.vocab.tgt exists, may overwrite...\n",
      "[2021-04-23 00:42:00,233 INFO] Parsed 2 corpora from -data.\n",
      "[2021-04-23 00:42:00,233 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n",
      "[2021-04-23 00:42:00,233 INFO] Loading vocab from text file...\n",
      "[2021-04-23 00:42:00,234 INFO] Loading src vocabulary from test_model_loose/save_data.vocab.src\n",
      "[2021-04-23 00:42:00,235 INFO] Loaded src vocab has 429 tokens.\n",
      "[2021-04-23 00:42:00,235 INFO] Loading tgt vocabulary from test_model_loose/save_data.vocab.tgt\n",
      "[2021-04-23 00:42:00,237 INFO] Loaded tgt vocab has 444 tokens.\n",
      "[2021-04-23 00:42:00,237 INFO] Building fields with vocab in counters...\n",
      "[2021-04-23 00:42:00,238 INFO]  * tgt vocab size: 448.\n",
      "[2021-04-23 00:42:00,238 INFO]  * src vocab size: 431.\n",
      "[2021-04-23 00:42:00,238 INFO]  * src vocab size = 431\n",
      "[2021-04-23 00:42:00,238 INFO]  * tgt vocab size = 448\n",
      "[2021-04-23 00:42:00,239 INFO] Building model...\n",
      "[2021-04-23 00:42:01,400 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(431, 512, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(512, 256, num_layers=2, dropout=0.2)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(448, 512, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(768, 256)\n",
      "        (1): LSTMCell(256, 256)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_context): Linear(in_features=256, out_features=256, bias=False)\n",
      "      (linear_query): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (v): Linear(in_features=256, out_features=1, bias=False)\n",
      "      (linear_out): Linear(in_features=512, out_features=256, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=448, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-04-23 00:42:01,400 INFO] encoder: 1535488\n",
      "[2021-04-23 00:42:01,400 INFO] decoder: 2184384\n",
      "[2021-04-23 00:42:01,400 INFO] * number of parameters: 3719872\n",
      "[2021-04-23 00:42:01,401 INFO] Starting training on GPU: [0]\n",
      "[2021-04-23 00:42:01,401 INFO] Start training loop and validate every 100 steps...\n",
      "[2021-04-23 00:42:01,402 INFO] corpus_1's transforms: TransformPipe()\n",
      "[2021-04-23 00:42:01,402 INFO] Loading ParallelCorpus(../data/small/abstract_methods/train_buggy.txt, ../data/small/edit_ops/loose/train.txt, align=None)...\n",
      "[2021-04-23 00:42:11,086 INFO] Step 50/  500; acc:  19.02; ppl: 149.80; xent: 5.01; lr: 0.00010; 10499/4189 tok/s;     10 sec\n",
      "[2021-04-23 00:42:20,980 INFO] Step 100/  500; acc:  25.66; ppl: 34.85; xent: 3.55; lr: 0.00010; 10178/4020 tok/s;     20 sec\n",
      "[2021-04-23 00:42:20,981 INFO] valid's transforms: TransformPipe()\n",
      "[2021-04-23 00:42:20,982 INFO] Loading ParallelCorpus(../data/small/abstract_methods/valid_buggy.txt, ../data/small/edit_ops/loose/valid.txt, align=None)...\n",
      "[2021-04-23 00:42:28,903 INFO] Validation perplexity: 26.2731\n",
      "[2021-04-23 00:42:28,903 INFO] Validation accuracy: 27.0358\n",
      "[2021-04-23 00:42:28,906 INFO] Saving checkpoint test_model_loose/model_step_100.pt\n",
      "[2021-04-23 00:42:39,452 INFO] Step 150/  500; acc:  28.72; ppl: 25.62; xent: 3.24; lr: 0.00010; 5513/2224 tok/s;     38 sec\n",
      "[2021-04-23 00:42:48,512 INFO] Step 200/  500; acc:  39.35; ppl: 17.74; xent: 2.88; lr: 0.00010; 11036/4409 tok/s;     47 sec\n",
      "[2021-04-23 00:42:48,513 INFO] Loading ParallelCorpus(../data/small/abstract_methods/valid_buggy.txt, ../data/small/edit_ops/loose/valid.txt, align=None)...\n",
      "[2021-04-23 00:42:56,440 INFO] Validation perplexity: 12.6475\n",
      "[2021-04-23 00:42:56,440 INFO] Validation accuracy: 45.342\n",
      "[2021-04-23 00:42:56,442 INFO] Saving checkpoint test_model_loose/model_step_200.pt\n",
      "[2021-04-23 00:43:06,535 INFO] Step 250/  500; acc:  45.08; ppl: 11.74; xent: 2.46; lr: 0.00010; 5684/2231 tok/s;     65 sec\n",
      "[2021-04-23 00:43:15,860 INFO] Step 300/  500; acc:  45.63; ppl:  9.98; xent: 2.30; lr: 0.00010; 10778/4211 tok/s;     74 sec\n",
      "[2021-04-23 00:43:15,862 INFO] Loading ParallelCorpus(../data/small/abstract_methods/valid_buggy.txt, ../data/small/edit_ops/loose/valid.txt, align=None)...\n",
      "[2021-04-23 00:43:23,783 INFO] Validation perplexity: 9.10963\n",
      "[2021-04-23 00:43:23,783 INFO] Validation accuracy: 46.1768\n",
      "[2021-04-23 00:43:23,785 INFO] Saving checkpoint test_model_loose/model_step_300.pt\n",
      "[2021-04-23 00:43:34,220 INFO] Step 350/  500; acc:  45.65; ppl:  9.50; xent: 2.25; lr: 0.00010; 5662/2199 tok/s;     93 sec\n",
      "[2021-04-23 00:43:44,069 INFO] Step 400/  500; acc:  45.68; ppl:  9.27; xent: 2.23; lr: 0.00010; 10412/4126 tok/s;    103 sec\n",
      "[2021-04-23 00:43:44,070 INFO] Loading ParallelCorpus(../data/small/abstract_methods/valid_buggy.txt, ../data/small/edit_ops/loose/valid.txt, align=None)...\n",
      "[2021-04-23 00:43:51,990 INFO] Validation perplexity: 8.46225\n",
      "[2021-04-23 00:43:51,990 INFO] Validation accuracy: 46.5759\n",
      "[2021-04-23 00:43:51,992 INFO] Saving checkpoint test_model_loose/model_step_400.pt\n",
      "[2021-04-23 00:44:02,389 INFO] Step 450/  500; acc:  45.98; ppl:  8.88; xent: 2.18; lr: 0.00010; 5523/2174 tok/s;    121 sec\n",
      "[2021-04-23 00:44:12,108 INFO] Step 500/  500; acc:  46.08; ppl:  8.87; xent: 2.18; lr: 0.00010; 10587/4161 tok/s;    131 sec\n",
      "[2021-04-23 00:44:12,109 INFO] Loading ParallelCorpus(../data/small/abstract_methods/valid_buggy.txt, ../data/small/edit_ops/loose/valid.txt, align=None)...\n",
      "[2021-04-23 00:44:20,029 INFO] Validation perplexity: 8.1738\n",
      "[2021-04-23 00:44:20,029 INFO] Validation accuracy: 47.3225\n",
      "[2021-04-23 00:44:20,031 INFO] Saving checkpoint test_model_loose/model_step_500.pt\n"
     ]
    }
   ],
   "source": [
    "# collapse_output\n",
    "model.train(\n",
    "    DATA_SMALL_METHODS_TRAIN_BUGGY,\n",
    "    DATA_SMALL_OPS_LOOSE_TRAIN,\n",
    "    DATA_SMALL_METHODS_VALID_BUGGY,\n",
    "    DATA_SMALL_OPS_LOOSE_VALID,\n",
    "    numCheckpoints = 5,\n",
    "    numTrainingSteps = 500,\n",
    "    numValidations = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to view information about the training process of the model without having to scroll through all the output above; we can use the `HephaestusModel.getTrainingStats` method, which returns a Pandas DataFrame containing such information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>trainAccuracy</th>\n",
       "      <th>validAccuracy</th>\n",
       "      <th>crossEntropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>19.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>25.66</td>\n",
       "      <td>27.0358</td>\n",
       "      <td>3.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150</td>\n",
       "      <td>28.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200</td>\n",
       "      <td>39.35</td>\n",
       "      <td>45.3420</td>\n",
       "      <td>2.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>250</td>\n",
       "      <td>45.08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>300</td>\n",
       "      <td>45.63</td>\n",
       "      <td>46.1768</td>\n",
       "      <td>2.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>350</td>\n",
       "      <td>45.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>400</td>\n",
       "      <td>45.68</td>\n",
       "      <td>46.5759</td>\n",
       "      <td>2.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>450</td>\n",
       "      <td>45.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>500</td>\n",
       "      <td>46.08</td>\n",
       "      <td>47.3225</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   step  trainAccuracy  validAccuracy  crossEntropy\n",
       "0    50          19.02            NaN          5.01\n",
       "1   100          25.66        27.0358          3.55\n",
       "2   150          28.72            NaN          3.24\n",
       "3   200          39.35        45.3420          2.88\n",
       "4   250          45.08            NaN          2.46\n",
       "5   300          45.63        46.1768          2.30\n",
       "6   350          45.65            NaN          2.25\n",
       "7   400          45.68        46.5759          2.23\n",
       "8   450          45.98            NaN          2.18\n",
       "9   500          46.08        47.3225          2.18"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.getTrainingStats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can test it out. This gets the first buggy `AbstractMethod` from the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "private TYPE_1 getType ( TYPE_2 VAR_1 ) { TYPE_3 VAR_2 = new TYPE_3 ( STRING_1 ) ; return new TYPE_1 ( VAR_2 , VAR_2 ) ; }"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buggyMethod = readAbstractMethodsFromFile(DATA_SMALL_METHODS_TEST_BUGGY)[0]\n",
    "buggyMethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then translate the method into a supposedly fixed version using `HephaestusModel.translate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-04-23 00:45:21,047 INFO] Translating shard 0.\n",
      "[2021-04-23 00:45:21,058 INFO] PRED AVG SCORE: -1.3559, PRED PPL: 3.8803\n"
     ]
    }
   ],
   "source": [
    "#collapse_output\n",
    "outputMethod = model.translate(buggyMethod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a possibility that the model was unable to translate the buggy method correctly, e.g. if the model outputted ill-formed EditOperations that could not be parsed and applied to the buggy method. Therefore, we should check that the outputted method is not `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(outputMethod is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the contents of the outputted `AbstractMethod`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "private TYPE_1 getType ( TYPE_2 VAR_1 ) { return new TYPE_1 ( VAR_2 , VAR_2 ) ; }"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputMethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can determine what exactly changed from the buggy method to the outputted method by getting the EditOperations between the two, then condensing them for easier readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[COMPOUND_DELETE 8:17]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observedOperations = getCondensedLoose(buggyMethod.getEditOperationsTo(outputMethod))\n",
    "observedOperations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems that the changes were deletions on tokens in the index range 8:17. We can verify that these were the actual edit operations applied by the model by looking at `raw_output.txt` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[COMPOUND_DELETE 8:17]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appliedOperations = readCompoundOperationsFromFile(\"test_model_loose/raw_output.txt\")[0]\n",
    "appliedOperations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appliedOperations == observedOperations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! But what was the correct answer, and how far off were we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "private TYPE_1 getType ( TYPE_2 VAR_1 ) { TYPE_3 VAR_2 = new TYPE_3 ( STRING_1 ) ; return new TYPE_1 ( VAR_2 , VAR_2 , this , VAR_1 ) ; }"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actualFixedMethod = readAbstractMethodsFromFile(DATA_SMALL_METHODS_TEST_FIXED)[0]\n",
    "actualFixedMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelDistance = outputMethod.getEditDistanceTo(actualFixedMethod)\n",
    "modelDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actualDistance = buggyMethod.getEditDistanceTo(actualFixedMethod)\n",
    "actualDistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `modelDistance` is higher than `actualDistance`, our outputted method is actually further away from the actual fixed method than the original buggy method is! Oof. But keep in mind that this is only demonstrating example usage and that the model was trained with a laughable number of steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
