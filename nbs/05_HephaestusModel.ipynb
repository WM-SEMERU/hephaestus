{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp HephaestusModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#hide\n",
    "from typing import Union, List, Optional, Tuple\n",
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from hephaestus.EditOperations import *\n",
    "from hephaestus.CondenseEditOperations import *\n",
    "from hephaestus.AbstractMethod import *\n",
    "from hephaestus.IOUtils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HephaestusModel\n",
    "\n",
    "> Encapsulates NMT operations on AbstractMethods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HephaestusModel:\n",
    "    \"\"\"\n",
    "    The `HephaestusModel` is the means through which buggy AbstractMethods are translated into fixed ones. Each\n",
    "    `HephaestusModel` occupies a directory which contains stored models, vocabularies, and configuration files.\n",
    "\n",
    "    Required args:\n",
    "    - `modelDir`: The directory which stores files pertaining to the model. You can use a directory which already\n",
    "      contains the necessary files (previously generated from a different `HephaestusModel`), in which case the\n",
    "      model will not have to be trained again. If you provide a directory that does not exist, the `HephaestsuModel`\n",
    "      will attempt to create it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, modelDir: str) -> None:\n",
    "\n",
    "        # set up constants\n",
    "        self.__MODEL_DIR =         modelDir if os.path.sep == \"/\" else modelDir.replace(\"/\", os.path.sep)\n",
    "        self.__CONFIG_PATH =       os.path.join(self.__MODEL_DIR, \"config.yaml\")\n",
    "        self.__SAVE_DATA_PATH =    os.path.join(self.__MODEL_DIR, \"save_data\")\n",
    "        self.__SOURCE_VOCAB_PATH = os.path.join(self.__MODEL_DIR, \"save_data.vocab.src\")\n",
    "        self.__TARGET_VOCAB_PATH = os.path.join(self.__MODEL_DIR, \"save_data.vocab.tgt\")\n",
    "        self.__RAW_OUTPUT_PATH =   os.path.join(self.__MODEL_DIR, \"raw_output.txt\")\n",
    "        self.__POST_OUTPUT_PATH =  os.path.join(self.__MODEL_DIR, \"postprocessed_output.txt\")\n",
    "        self.__SAVE_MODEL_PREFIX = \"model\"\n",
    "        self.__SAVE_MODEL_PATH =   os.path.join(self.__MODEL_DIR, self.__SAVE_MODEL_PREFIX)\n",
    "        self.__FINAL_MODEL_PATH =  os.path.join(self.__MODEL_DIR, self.__SAVE_MODEL_PREFIX + \"_final.pt\")\n",
    "\n",
    "        # create the modelDir directory if it doesn't already exist\n",
    "        if not os.path.isdir(self.__MODEL_DIR):\n",
    "            os.mkdir(self.__MODEL_DIR)\n",
    "\n",
    "    def train(self,\n",
    "\n",
    "        trainSource: str,\n",
    "        trainTarget: str,\n",
    "        validSource: str,\n",
    "        validTarget: str,\n",
    "\n",
    "        vocabSamples: int =       10000,\n",
    "        numGPUs: int =                1,\n",
    "        trainSteps: int =          1000,\n",
    "        validSteps: int =          None,\n",
    "        saveCheckpointSteps: int = None\n",
    "\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Trains the model with the given parameters. Files containing AbstractMethods should have one per line with\n",
    "        tokens separated by spaces. 'source' files must contain AbstractMethods. 'target' files may contain\n",
    "        AbstractMethods or CompoundOperations.\n",
    "\n",
    "        As the training progesses, checkpoint model files are created which follow the format `model_step_#.pt`, where\n",
    "        `#` corresponds to the training step number. Once training is complete, the finalized model is outputted to\n",
    "        `model_final.pt`.\n",
    "\n",
    "        Required args:\n",
    "        - `trainSource`: File name contatining training source data, must be buggy AbstractMethods.\n",
    "        - `trainTarget`: File name contatining training target data, can be fixed AbstractMethods or CompoundOperations\n",
    "          describing buggy -> fixed.\n",
    "        - `validSource`: File name contatining validation source data, must be buggy AbstractMethods.\n",
    "        - `validTarget`: File name contatining validation target data, must be the same type of data provided in\n",
    "          `trainTarget`.\n",
    "\n",
    "        Optional args:\n",
    "        - `vocabSamples`: Number of transformed samples per corpus to use when building vocabulary. Defaults to 10000.\n",
    "        - `numGPUs`: Number of GPUs to use concurrently during training. If set to 0, then the CPU is used. Defaults to\n",
    "          1.\n",
    "        - `trainSteps`: Number of training steps to go through. Defaults to 1000.\n",
    "        - `validSteps`: Number of training steps after which each validation occurs; e.g. if `trainSteps` is 1000 and\n",
    "          `validSteps` is 500, then validation will occur after training steps 500 and 1000. Defaults to\n",
    "          `trainSteps` / 2.\n",
    "        - `saveCheckpointSteps`: Number of training steps after which model checkpoints are saved; e.g. if `trainSteps`\n",
    "          is 1000 and 'saveCheckpointSteps' is 500, then the model will be saved in after training steps 5000 and 1000.\n",
    "          Defaults to `trainSteps` / 2.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Right now this is just the default model, we should tune the parameters so that it's\n",
    "        # somewhat specialized to our use case and it works better.\n",
    "\n",
    "        # determine number of validation and checkpoint steps if none were given\n",
    "        if validSteps is None:\n",
    "            validSteps = max(trainSteps // 2, 1)\n",
    "        if saveCheckpointSteps is None:\n",
    "            saveCheckpointSteps = max(trainSteps // 2, 1)\n",
    "\n",
    "        # write config file\n",
    "        self.__writeConfigFile(trainSource, trainTarget, validSource, validTarget, numGPUs = numGPUs,\n",
    "                trainSteps = trainSteps, validSteps = validSteps, saveCheckpointSteps = saveCheckpointSteps)\n",
    "        \n",
    "        # build vocabulary\n",
    "        runCommand('onmt_build_vocab -config \"{}\" -n_sample {}'.format(self.__CONFIG_PATH, vocabSamples))\n",
    "\n",
    "        # delete previous model files\n",
    "        for file in os.listdir(self.__MODEL_DIR):\n",
    "            if re.search(r\"^\" + self.__SAVE_MODEL_PREFIX + r\"_(?:step_[0-9]+|final).pt$\", file):\n",
    "                os.remove(os.path.join(self.__MODEL_DIR, file))\n",
    "    \n",
    "        # train the model\n",
    "        runCommand('onmt_train -config \"{}\"'.format(self.__CONFIG_PATH))\n",
    "\n",
    "        # find and release the highest trained model\n",
    "        latestModel = None\n",
    "        maxNum = 0\n",
    "        for file in os.listdir(self.__MODEL_DIR):\n",
    "            match = re.search(r\"^\" + self.__SAVE_MODEL_PREFIX + r\"_(?:step_([0-9]+)|final).pt$\", file)\n",
    "            if match:\n",
    "                stepNum = int(match.group(1))\n",
    "                if stepNum > maxNum:\n",
    "                    latestModel = os.path.join(self.__MODEL_DIR, file)\n",
    "                    maxNum = stepNum\n",
    "\n",
    "        if latestModel is not None:\n",
    "            runCommand('onmt_release_model --model \"{}\" --output \"{}\"'.format(latestModel, self.__FINAL_MODEL_PATH))\n",
    "\n",
    "    def translate(self,\n",
    "        buggy: Union[str, AbstractMethod, List[AbstractMethod]],\n",
    "        modelFile: str = None,\n",
    "        applyEditOperations: bool = True\n",
    "    ) -> Union[Optional[AbstractMethod], List[Optional[AbstractMethod]]]:\n",
    "        \"\"\"\n",
    "        Translates the given `buggy` AbstractMethods into supposedly fixed AbstractMethods, writes them to\n",
    "        `<model_directory>/postprocessed_output.txt`, and then returns them. The raw output of the model is written\n",
    "        to `<model_directory>/raw_output.txt` in case you want to access that as well. Depending on what type of\n",
    "        value is passed to `buggy`, the return value of this method changes according to the following:\n",
    "\n",
    "        | `buggy` type           | Return type                      |\n",
    "        | :--------------------- | :------------------------------- |\n",
    "        | `str` (a file)         | `List[Optional[AbstractMethod]]` |\n",
    "        | `AbstractMethod`       | `Optional[AbstractMethod]`       |\n",
    "        | `List[AbstractMethod]` | `List[Optional[AbstractMethod]]` |\n",
    "\n",
    "        A `None` return value means that the model was unable to translate that abstract method correctly. This\n",
    "        could be due to the model outputting non well-formed CompoundOperations, among other things. These will\n",
    "        appear as blank lines in `postprocessed_output.txt`.\n",
    "\n",
    "        Optional args:\n",
    "        - `modelFile`: A `.pt` file which is used for translation instead of the default `model_final.pt`\n",
    "        - `applyEditOperations`: When set to True, the model output is interpreted as CompoundOperations and a\n",
    "          postprocessing stage occurs where the outputted CompoundOperations are applied to the inputted\n",
    "          AbstractMethods. When set to False, the raw output is interpreted as AbstractMethods and returned\n",
    "          without a postprocessing stage; in this case, the contents of `raw_output.txt` and\n",
    "          `postprocessed_output.txt` are identical. If the model was trained with EditOperations,\n",
    "          `applyEditOperations` should be True; if the model was trained with just AbstractMethods as in for\n",
    "          the control group, then this should be False. Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        # determine which model file to use, and raise an error if it doesn't exist\n",
    "        if modelFile is None:\n",
    "            modelFile = self.__FINAL_MODEL_PATH\n",
    "        if not os.path.isfile(modelFile):\n",
    "            raise FileNotFoundError(\"Hephaestus: model not found -- {}\".format(modelFile))\n",
    "        \n",
    "        # write the AbstractMethods to a file if they were given directly\n",
    "        buggyFile = None\n",
    "        if type(buggy) in (AbstractMethod, list):\n",
    "            buggyFile = os.path.join(self.__MODEL_DIR, \"input.txt\")\n",
    "            writeAbstractMethodsToFile(buggyFile, buggy if type(buggy) is list else [buggy])\n",
    "        else:\n",
    "            buggyFile = buggy\n",
    "        \n",
    "        # translate the buggy methods\n",
    "        command = 'onmt_translate -model \"{}\" -src \"{}\" -output \"{}\"'.format(modelFile, buggyFile, self.__RAW_OUTPUT_PATH)\n",
    "        if getYamlParameter(self.__CONFIG_PATH, \"world_size\") is not None: # if GPU should be used\n",
    "            command += \"-gpu 0\"\n",
    "        runCommand(command)\n",
    "\n",
    "        # strip the last line of the output file, as OpenNMT likes to put a newline at the end\n",
    "        with open(self.__RAW_OUTPUT_PATH, \"r+\") as outputFile:\n",
    "            lines = outputFile.readlines()\n",
    "            lines[-1] = lines[-1].strip()\n",
    "            outputFile.seek(0)\n",
    "            outputFile.writelines(lines)\n",
    "            outputFile.truncate()\n",
    "\n",
    "        # get all inputted AbstractMethods\n",
    "        inputMethods = []\n",
    "        if type(buggy) in (AbstractMethod, list):\n",
    "            inputMethods = buggy if type(buggy) is list else [buggy]\n",
    "        else:\n",
    "            inputMethods = readAbstractMethodsFromFile(buggyFile)\n",
    "        \n",
    "        # If edit ops should be applied, then extract the operations from the output file and attempt to\n",
    "        # apply them to the input methods. Assign a None value to a fixed method if its corresponding\n",
    "        # operations were not able to be read. Copy input methods before applying edit operations so that\n",
    "        # the original remains unmodified.\n",
    "        fixedMethods = []\n",
    "        if applyEditOperations:\n",
    "            operations = readCompoundOperationsFromFile(self.__RAW_OUTPUT_PATH)\n",
    "            for inputMethod, opList in zip(inputMethods, operations):\n",
    "                if opList is None:\n",
    "                    fixedMethods.append(None)\n",
    "                else:\n",
    "                    try:\n",
    "                        inputMethodCopy = deepcopy(inputMethod)\n",
    "                        inputMethodCopy.applyEditOperations(opList)\n",
    "                        fixedMethods.append(inputMethodCopy)\n",
    "                    except:\n",
    "                        fixedMethods.append(None)\n",
    "        \n",
    "        # Simply interpret the output as abstract methods if not interpreting as edit operations\n",
    "        else:\n",
    "            fixedMethods = readAbstractMethodsFromFile(self.__RAW_OUTPUT_PATH)\n",
    "        \n",
    "        # substitute None for empty AbstractMethods and write the fixed methods to the postprocessed output file\n",
    "        writeAbstractMethodsToFile(\n",
    "            self.__POST_OUTPUT_PATH,\n",
    "            [\" \" if method is None else method for method in fixedMethods]\n",
    "        )\n",
    "\n",
    "        # return fixed methods\n",
    "        return fixedMethods if type(buggy) is list else fixedMethods[0]\n",
    "\n",
    "    def __writeConfigFile(self, *args, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Creates the config file.\n",
    "        \"\"\"\n",
    "        \n",
    "        lines = [\n",
    "            \"# AUTOGENERATED\",\n",
    "            \"\",\n",
    "            \"# Samples will be written to here\",\n",
    "            \"save_data: {}\".format(self.__SAVE_DATA_PATH.replace(os.path.sep, \"/\")),\n",
    "            \"\",\n",
    "            \"# Vocabs will be written to these files\",\n",
    "            \"src_vocab: {}\".format(self.__SOURCE_VOCAB_PATH.replace(os.path.sep, \"/\")),\n",
    "            \"tgt_vocab: {}\".format(self.__TARGET_VOCAB_PATH.replace(os.path.sep, \"/\")),\n",
    "            \"\",\n",
    "            \"# Allow overwriting existing files in the directory\",\n",
    "            \"overwrite: True\",\n",
    "            \"\",\n",
    "            \"# Data corpus\",\n",
    "            \"data:\",\n",
    "            \"    corpus_1:\",\n",
    "            \"        path_src: {}\".format(args[0].replace(os.path.sep, \"/\")),\n",
    "            \"        path_tgt: {}\".format(args[1].replace(os.path.sep, \"/\")),\n",
    "            \"        transforms: []\",\n",
    "            \"        weight: 1\",\n",
    "            \"    valid:\",\n",
    "            \"        path_src: {}\".format(args[2].replace(os.path.sep, \"/\")),\n",
    "            \"        path_tgt: {}\".format(args[3].replace(os.path.sep, \"/\")),\n",
    "            \"        transforms: []\",\n",
    "            \"\",\n",
    "            \"# Checkpoints will be saved here\",\n",
    "            \"save_model: {}\".format(self.__SAVE_MODEL_PATH.replace(os.path.sep, \"/\")),\n",
    "            \"save_checkpoint_steps: {}\".format(kwargs[\"saveCheckpointSteps\"]),\n",
    "            \"train_steps: {}\".format(kwargs[\"trainSteps\"]),\n",
    "            \"valid_steps: {}\".format(kwargs[\"validSteps\"]),\n",
    "            \"\"\n",
    "        ]\n",
    "\n",
    "        numGPUs = kwargs[\"numGPUs\"]\n",
    "        if numGPUs > 0:\n",
    "            lines += [\n",
    "                \"# Train using {} GPU{}\".format(numGPUs, \"s\" if numGPUs > 1 else \"\"),\n",
    "                \"world_size: {}\".format(numGPUs),\n",
    "                \"gpu_ranks:\",\n",
    "                *[\"- {}\".format(i) for i in range(numGPUs)]\n",
    "            ]\n",
    "        else:\n",
    "            lines += [\"# Train using the CPU, so no world_size provided\"]\n",
    "\n",
    "        with open(self.__CONFIG_PATH, \"w\") as file:\n",
    "            file.write(\"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"HephaestusModel.train\" class=\"doc_header\"><code>HephaestusModel.train</code><a href=\"__main__.py#L32\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>HephaestusModel.train</code>(**`trainSource`**:`str`, **`trainTarget`**:`str`, **`validSource`**:`str`, **`validTarget`**:`str`, **`vocabSamples`**:`int`=*`10000`*, **`numGPUs`**:`int`=*`1`*, **`trainSteps`**:`int`=*`1000`*, **`validSteps`**:`int`=*`None`*, **`saveCheckpointSteps`**:`int`=*`None`*)\n\nTrains the model with the given parameters. Files containing AbstractMethods should have one per line with\ntokens separated by spaces. 'source' files must contain AbstractMethods. 'target' files may contain\nAbstractMethods or CompoundOperations.\n\nAs the training progesses, checkpoint model files are created which follow the format `model_step_#.pt`, where\n`#` corresponds to the training step number. Once training is complete, the finalized model is outputted to\n`model_final.pt`.\n\nRequired args:\n- `trainSource`: File name contatining training source data, must be buggy AbstractMethods.\n- `trainTarget`: File name contatining training target data, can be fixed AbstractMethods or CompoundOperations\n  describing buggy -> fixed.\n- `validSource`: File name contatining validation source data, must be buggy AbstractMethods.\n- `validTarget`: File name contatining validation target data, must be the same type of data provided in\n  `trainTarget`.\n\nOptional args:\n- `vocabSamples`: Number of transformed samples per corpus to use when building vocabulary. Defaults to 10000.\n- `numGPUs`: Number of GPUs to use concurrently during training. If set to 0, then the CPU is used. Defaults to\n  1.\n- `trainSteps`: Number of training steps to go through. Defaults to 1000.\n- `validSteps`: Number of training steps after which each validation occurs; e.g. if `trainSteps` is 1000 and\n  `validSteps` is 500, then validation will occur after training steps 500 and 1000. Defaults to\n  `trainSteps` / 2.\n- `saveCheckpointSteps`: Number of training steps after which model checkpoints are saved; e.g. if `trainSteps`\n  is 1000 and 'saveCheckpointSteps' is 500, then the model will be saved in after training steps 5000 and 1000.\n  Defaults to `trainSteps` / 2.",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(HephaestusModel.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"HephaestusModel.translate\" class=\"doc_header\"><code>HephaestusModel.translate</code><a href=\"__main__.py#L114\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>HephaestusModel.translate</code>(**`buggy`**:`Union`\\[`str`, [`AbstractMethod`](/hephaestus/AbstractMethod.html), `List`\\[[`AbstractMethod`](/hephaestus/AbstractMethod.html)\\]\\], **`modelFile`**:`str`=*`None`*, **`applyEditOperations`**:`bool`=*`True`*)\n\nTranslates the given `buggy` AbstractMethods into supposedly fixed AbstractMethods, writes them to\n`<model_directory>/postprocessed_output.txt`, and then returns them. The raw output of the model is written\nto `<model_directory/raw_output.txt>` in case you want to access that as well. Depending on what type of\nvalue is passed to `buggy`, the return value of this method changes according to the following:\n\n| `buggy` type           | Return type                      |\n| :--------------------- | :------------------------------- |\n| `str` (a file)         | `List[Optional[AbstractMethod]]` |\n| [`AbstractMethod`](/hephaestus/AbstractMethod.html)       | `Optional[AbstractMethod]`       |\n| `List[AbstractMethod]` | `List[Optional[AbstractMethod]]` |\n\nA `None` return value means that the model was unable to translate that abstract method correctly. This\ncould be due to the model outputting non well-formed CompoundOperations, among other things. These will\nappear as blank lines in `postprocessed_output.txt`.\n\nOptional args:\n- `modelFile`: A `.pt` file which is used for translation instead of the default `model_final.pt`\n- `applyEditOperations`: When set to True, the model output is interpreted as CompoundOperations and a\n  postprocessing stage occurs where the outputted CompoundOperations are applied to the inputted\n  AbstractMethods. When set to False, the raw output is interpreted as AbstractMethods and returned\n  without a postprocessing stage; in this case, the contents of `raw_output.txt` and\n  `postprocessed_output.txt` are identical. If the model was trained with EditOperations,\n  `applyEditOperations` should be True; if the model was trained with just AbstractMethods as in for\n  the control group, then this should be False. Defaults to True.",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(HephaestusModel.translate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Let's create a small test model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HephaestusModel(\"test_model_loose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there is a directory called `test_model_loose` which will be populated with files once the model is trained. It's time to train the model with the loosely condensed edit operations dataset. Since this is just an example, a *very* small number of training steps will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-04-19 05:48:46,172 INFO] Counter vocab from 10000 samples.\n",
      "[2021-04-19 05:48:46,172 INFO] Build vocab on 10000 transformed examples/corpus.\n",
      "[2021-04-19 05:48:48,403 INFO] Counters src:411\n",
      "[2021-04-19 05:48:48,403 INFO] Counters tgt:405\n",
      "[2021-04-19 05:48:50,852 WARNING] You have a CUDA device, should run with -gpu_ranks\n",
      "[2021-04-19 05:48:50,854 INFO] Parsed 2 corpora from -data.\n",
      "[2021-04-19 05:48:50,855 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n",
      "[2021-04-19 05:48:50,855 INFO] Loading vocab from text file...\n",
      "[2021-04-19 05:48:50,855 INFO] Loading src vocabulary from test_model_loose/save_data.vocab.src\n",
      "[2021-04-19 05:48:50,882 INFO] Loaded src vocab has 411 tokens.\n",
      "[2021-04-19 05:48:50,883 INFO] Loading tgt vocabulary from test_model_loose/save_data.vocab.tgt\n",
      "[2021-04-19 05:48:50,902 INFO] Loaded tgt vocab has 405 tokens.\n",
      "[2021-04-19 05:48:50,902 INFO] Building fields with vocab in counters...\n",
      "[2021-04-19 05:48:50,904 INFO]  * tgt vocab size: 409.\n",
      "[2021-04-19 05:48:50,905 INFO]  * src vocab size: 413.\n",
      "[2021-04-19 05:48:50,907 INFO]  * src vocab size = 413\n",
      "[2021-04-19 05:48:50,908 INFO]  * tgt vocab size = 409\n",
      "[2021-04-19 05:48:50,909 INFO] Building model...\n",
      "[2021-04-19 05:48:51,166 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(413, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(500, 500, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(409, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(1000, 500)\n",
      "        (1): LSTMCell(500, 500)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (linear_out): Linear(in_features=1000, out_features=500, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=500, out_features=409, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-04-19 05:48:51,167 INFO] encoder: 4214500\n",
      "[2021-04-19 05:48:51,168 INFO] decoder: 6167409\n",
      "[2021-04-19 05:48:51,168 INFO] * number of parameters: 10381909\n",
      "[2021-04-19 05:48:51,170 INFO] Starting training on CPU, could be very slow\n",
      "[2021-04-19 05:48:51,170 INFO] Start training loop and validate every 100 steps...\n",
      "[2021-04-19 05:48:51,170 INFO] corpus_1's transforms: TransformPipe()\n",
      "[2021-04-19 05:48:51,171 INFO] Loading ParallelCorpus(../data/abstract_methods/small/train_buggy.txt, ../data/edit_ops/loose/small/train.txt, align=None)...\n",
      "[2021-04-19 05:54:27,531 INFO] Step 50/  500; acc:  17.13; ppl: 1350.45; xent: 7.21; lr: 1.00000; 311/120 tok/s;    336 sec\n",
      "[2021-04-19 05:59:31,824 INFO] Step 100/  500; acc:  29.31; ppl: 85.16; xent: 4.44; lr: 1.00000; 328/132 tok/s;    641 sec\n",
      "[2021-04-19 05:59:31,825 INFO] valid's transforms: TransformPipe()\n",
      "[2021-04-19 05:59:31,825 INFO] Loading ParallelCorpus(../data/abstract_methods/small/valid_buggy.txt, ../data/edit_ops/loose/small/valid.txt, align=None)...\n",
      "[2021-04-19 06:01:09,867 INFO] Validation perplexity: 21.7216\n",
      "[2021-04-19 06:01:09,867 INFO] Validation accuracy: 41.7856\n",
      "[2021-04-19 06:06:31,322 INFO] Step 150/  500; acc:  39.13; ppl: 21.94; xent: 3.09; lr: 1.00000; 248/ 98 tok/s;   1060 sec\n",
      "[2021-04-19 06:11:13,374 INFO] Step 200/  500; acc:  40.43; ppl: 15.15; xent: 2.72; lr: 1.00000; 352/141 tok/s;   1342 sec\n",
      "[2021-04-19 06:11:13,375 INFO] Loading ParallelCorpus(../data/abstract_methods/small/valid_buggy.txt, ../data/edit_ops/loose/small/valid.txt, align=None)...\n",
      "[2021-04-19 06:12:49,123 INFO] Validation perplexity: 10.3744\n",
      "[2021-04-19 06:12:49,123 INFO] Validation accuracy: 45.433\n",
      "[2021-04-19 06:17:52,490 INFO] Step 250/  500; acc:  44.47; ppl:  9.72; xent: 2.27; lr: 1.00000; 257/102 tok/s;   1741 sec\n",
      "[2021-04-19 06:17:52,498 INFO] Saving checkpoint test_model_loose/model_step_250.pt\n",
      "[2021-04-19 06:22:49,922 INFO] Step 300/  500; acc:  44.98; ppl: 10.27; xent: 2.33; lr: 1.00000; 340/131 tok/s;   2039 sec\n",
      "[2021-04-19 06:22:49,923 INFO] Loading ParallelCorpus(../data/abstract_methods/small/valid_buggy.txt, ../data/edit_ops/loose/small/valid.txt, align=None)...\n",
      "[2021-04-19 06:24:24,262 INFO] Validation perplexity: 7.96014\n",
      "[2021-04-19 06:24:24,262 INFO] Validation accuracy: 48.6433\n",
      "[2021-04-19 06:29:28,727 INFO] Step 350/  500; acc:  47.42; ppl:  8.03; xent: 2.08; lr: 1.00000; 255/101 tok/s;   2438 sec\n",
      "[2021-04-19 06:34:41,781 INFO] Step 400/  500; acc:  48.23; ppl:  7.83; xent: 2.06; lr: 1.00000; 331/132 tok/s;   2751 sec\n",
      "[2021-04-19 06:34:41,782 INFO] Loading ParallelCorpus(../data/abstract_methods/small/valid_buggy.txt, ../data/edit_ops/loose/small/valid.txt, align=None)...\n",
      "[2021-04-19 06:36:15,558 INFO] Validation perplexity: 6.6537\n",
      "[2021-04-19 06:36:15,558 INFO] Validation accuracy: 48.908\n",
      "[2021-04-19 06:41:06,723 INFO] Step 450/  500; acc:  50.66; ppl:  6.50; xent: 1.87; lr: 1.00000; 257/102 tok/s;   3136 sec\n",
      "[2021-04-19 06:46:14,273 INFO] Step 500/  500; acc:  50.98; ppl:  6.39; xent: 1.85; lr: 1.00000; 341/133 tok/s;   3443 sec\n",
      "[2021-04-19 06:46:14,273 INFO] Loading ParallelCorpus(../data/abstract_methods/small/valid_buggy.txt, ../data/edit_ops/loose/small/valid.txt, align=None)...\n",
      "[2021-04-19 06:47:48,556 INFO] Validation perplexity: 5.85019\n",
      "[2021-04-19 06:47:48,557 INFO] Validation accuracy: 53.8096\n",
      "[2021-04-19 06:47:48,564 INFO] Saving checkpoint test_model_loose/model_step_500.pt\n"
     ]
    }
   ],
   "source": [
    "# collapse_output\n",
    "model.train(\n",
    "    \"../data/abstract_methods/small/train_buggy.txt\",\n",
    "    \"../data/edit_ops/loose/small/train.txt\",\n",
    "    \"../data/abstract_methods/small/valid_buggy.txt\",\n",
    "    \"../data/edit_ops/loose/small/valid.txt\",\n",
    "    numGPUs = 0,\n",
    "    trainSteps = 500,\n",
    "    validSteps = 100,\n",
    "    saveCheckpointSteps = 250\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can test it out. This gets the first buggy `AbstractMethod` from the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "private TYPE_1 getType ( TYPE_2 VAR_1 ) { TYPE_3 VAR_2 = new TYPE_3 ( STRING_1 ) ; return new TYPE_1 ( VAR_2 , VAR_2 ) ; }"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buggyMethod = readAbstractMethodsFromFile(\"../data/abstract_methods/small/test_buggy.txt\")[0]\n",
    "buggyMethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then translate the method into a supposedly fixed version using `HephaestusModel.translate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-04-19 14:08:08,666 INFO] Translating shard 0.\n",
      "C:\\Users\\aidan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\onmt\\translate\\beam_search.py:275: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [1, 5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ..\\aten\\src\\ATen\\native\\Resize.cpp:19.)\n",
      "  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n",
      "[2021-04-19 14:08:08,776 INFO] PRED AVG SCORE: -0.6497, PRED PPL: 1.9150\n"
     ]
    }
   ],
   "source": [
    "#collapse_output\n",
    "outputMethod = model.translate(buggyMethod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "public TYPE_1 getType ( TYPE_2 VAR_1 ) { TYPE_3 VAR_2 = new TYPE_3 ( STRING_1 ) ; return new TYPE_1 ( VAR_2 , VAR_2 ) ; }"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputMethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm, the `buggyMethod` and `outputMethod` look very similar. What exactly changed from the former to the latter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[REPLACE 0 -> 'public']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "operations = buggyMethod.getEditOperationsTo(outputMethod)\n",
    "operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there was only one operation done on the inputted buggy method, which was to replace the token at index 0 with `\"public\"`, i.e. `\"private\"` was changed to `\"public\"`. We can verify that these were the actual edit operations applied by the model by looking at `raw_output.txt` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[COMPOUND_REPLACE 0:1 -> ['public']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compoundOps = readCompoundOperationsFromFile(\"test_model_loose/raw_output.txt\")[0]\n",
    "compoundOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compoundOps == getCondensedLoose(operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! But what was the correct answer, and how far off were we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "private TYPE_1 getType ( TYPE_2 VAR_1 ) { TYPE_3 VAR_2 = new TYPE_3 ( STRING_1 ) ; return new TYPE_1 ( VAR_2 , VAR_2 , this , VAR_1 ) ; }"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actualFixedMethod = readAbstractMethodsFromFile(\"../data/abstract_methods/small/test_fixed.txt\")[0]\n",
    "actualFixedMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelDistance = outputMethod.getEditDistanceTo(actualFixedMethod)\n",
    "modelDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actualDistance = buggyMethod.getEditDistanceTo(actualFixedMethod)\n",
    "actualDistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `modelDistance` is higher than `actualDistance`, our outputted method is actually further away from the actual fixed method than the original buggy method is! Oof. But keep in mind that this is only demonstrating example usage and that the model was trained with a laughable number of steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit (windows store)",
   "name": "python394jvsc74a57bd0643e2b53eabc93377e95b07774b60b49bf379f8a09a9dbf1d193d7cd19dee0e4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
