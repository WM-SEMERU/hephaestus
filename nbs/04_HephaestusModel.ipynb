{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp HephaestusModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#hide\n",
    "from typing import Union, List, Literal, Optional, Tuple\n",
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from hephaestus.EditOperations import *\n",
    "from hephaestus.CondenseEditOperations import *\n",
    "from hephaestus.AbstractMethod import *\n",
    "from hephaestus.IOUtils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HephaestusModel\n",
    "\n",
    "> Encapsulates NMT operations on AbstractMethods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HephaestusModel:\n",
    "    \"\"\"\n",
    "    The `HephaestusModel` is the means through which buggy AbstractMethods are translated into fixed ones. Each\n",
    "    `HephaestusModel` occupies a directory which contains stored models, vocabularies, and configuration files.\n",
    "\n",
    "    Required args:\n",
    "    - `modelDir`: The directory which stores files pertaining to the model. You can use a directory which already\n",
    "      contains the necessary files (previously generated from a different `HephaestusModel`), in which case the\n",
    "      model will not have to be trained again. If you provide a directory that does not exist, the `HephaestsuModel`\n",
    "      will attempt to create it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, modelDir: str) -> None:\n",
    "\n",
    "        # set up constants\n",
    "        self.__MODEL_DIR =         modelDir if os.path.sep == \"/\" else modelDir.replace(\"/\", os.path.sep)\n",
    "        self.__CONFIG_PATH =       os.path.join(self.__MODEL_DIR, \"config.yaml\")\n",
    "        self.__SAVE_DATA_PATH =    os.path.join(self.__MODEL_DIR, \"save_data\")\n",
    "        self.__SOURCE_VOCAB_PATH = os.path.join(self.__MODEL_DIR, \"save_data.vocab.src\")\n",
    "        self.__TARGET_VOCAB_PATH = os.path.join(self.__MODEL_DIR, \"save_data.vocab.tgt\")\n",
    "        self.__OUTPUT_PATH =       os.path.join(self.__MODEL_DIR, \"output.txt\")\n",
    "        self.__SAVE_MODEL_PREFIX = \"model\"\n",
    "        self.__SAVE_MODEL_PATH =   os.path.join(self.__MODEL_DIR, self.__SAVE_MODEL_PREFIX)\n",
    "        self.__FINAL_MODEL_PATH =  os.path.join(self.__MODEL_DIR, self.__SAVE_MODEL_PREFIX + \"_final.pt\")\n",
    "\n",
    "        # create the modelDir directory if it doesn't already exist\n",
    "        if not os.path.isdir(self.__MODEL_DIR):\n",
    "            os.mkdir(self.__MODEL_DIR)\n",
    "\n",
    "    def train(self,\n",
    "\n",
    "        trainSource: str,\n",
    "        trainTarget: str,\n",
    "        validSource: str,\n",
    "        validTarget: str,\n",
    "\n",
    "        vocabSamples: int =       10000,\n",
    "        numGPUs: int =                1,\n",
    "        trainSteps: int =          1000,\n",
    "        validSteps: int =          None,\n",
    "        saveCheckpointSteps: int = None\n",
    "\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Trains the model with the given parameters. Files containing AbstractMethods should have one per line with\n",
    "        tokens separated by spaces. 'source' files must contain AbstractMethods. 'target' files may contain\n",
    "        AbstractMethods or EditOperations.\n",
    "\n",
    "        As the training progesses, checkpoint model files are created which follow the format `model_step_#.pt`, where\n",
    "        `#` corresponds to the training step number. Once training is complete, the finalized model is outputted to\n",
    "        `model_final.pt`.\n",
    "\n",
    "        Required args:\n",
    "        - `trainSource`: File name contatining training source data, must be buggy AbstractMethods.\n",
    "        - `trainTarget`: File name contatining training target data, can be fixed AbstractMethods or EditOperations\n",
    "          describing buggy -> fixed.\n",
    "        - `validSource`: File name contatining validation source data, must be buggy AbstractMethods.\n",
    "        - `validTarget`: File name contatining validation target data, must be the same type of data provided in\n",
    "          `trainTarget`.\n",
    "\n",
    "        Optional args:\n",
    "        - `vocabSamples`: Number of transformed samples per corpus to use when building vocabulary. Defaults to 10000.\n",
    "        - `numGPUs`: Number of GPUs to use concurrently during training. If set to 0, then the CPU is used. Defaults to\n",
    "          1.\n",
    "        - `trainSteps`: Number of training steps to go through. Defaults to 1000.\n",
    "        - `validSteps`: Number of training steps after which each validation occurs; e.g. if `trainSteps` is 1000 and\n",
    "          `validSteps` is 500, then validation will occur after training steps 500 and 1000. Defaults to\n",
    "          `trainSteps` / 2.\n",
    "        - `saveCheckpointSteps`: Number of training steps after which model checkpoints are saved; e.g. if `trainSteps`\n",
    "          is 1000 and 'saveCheckpointSteps' is 500, then the model will be saved in after training steps 5000 and 1000.\n",
    "          Defaults to `trainSteps` / 2.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Right now this is just the default model, we should tune the parameters so that it's\n",
    "        # somewhat specialized to our use case and it works better.\n",
    "\n",
    "        # determine number of validation and checkpoint steps if none were given\n",
    "        if validSteps is None:\n",
    "            validSteps = max(trainSteps // 2, 1)\n",
    "        if saveCheckpointSteps is None:\n",
    "            saveCheckpointSteps = max(trainSteps // 2, 1)\n",
    "\n",
    "        # write config file\n",
    "        self.__writeConfigFile(trainSource, trainTarget, validSource, validTarget, numGPUs = numGPUs,\n",
    "                trainSteps = trainSteps, validSteps = validSteps, saveCheckpointSteps = saveCheckpointSteps)\n",
    "        \n",
    "        # build vocabulary\n",
    "        runCommand([\"onmt_build_vocab -config '{}' -n_sample {}\".format(self.__CONFIG_PATH, vocabSamples)])\n",
    "\n",
    "        # delete previous model files\n",
    "        for file in os.listdir(self.__MODEL_DIR):\n",
    "            if re.search(r\"^\" + self.__SAVE_MODEL_PREFIX + r\"_(?:step_[0-9]+|final).pt$\", file):\n",
    "                os.remove(os.path.join(self.__MODEL_DIR, file))\n",
    "    \n",
    "        # train the model\n",
    "        runCommand([\"onmt_train -config '{}'\".format(self.__CONFIG_PATH)])\n",
    "\n",
    "        # find and release the highest trained model\n",
    "        latestModel = None\n",
    "        maxNum = 0\n",
    "        for file in os.listdir(self.__MODEL_DIR):\n",
    "            match = re.search(r\"^\" + self.__SAVE_MODEL_PREFIX + r\"_(?:step_([0-9]+)|final).pt$\", file)\n",
    "            if match:\n",
    "                stepNum = int(match.group(1))\n",
    "                if stepNum > maxNum:\n",
    "                    latestModel = os.path.join(self.__MODEL_DIR, file)\n",
    "                    maxNum = stepNum\n",
    "\n",
    "        if latestModel is not None:\n",
    "            runCommand([\"onmt_release_model --model '{}' --output '{}'\".format(latestModel, self.__FINAL_MODEL_PATH)])\n",
    "\n",
    "    def translate(self,\n",
    "        buggy: Union[str, AbstractMethod, List[AbstractMethod]],\n",
    "        modelFile: str = None,\n",
    "        applyEditOperations: bool = True\n",
    "    ) -> Union[AbstractMethod, List[AbstractMethod]]:\n",
    "        \"\"\"\n",
    "        Translates the given `buggy` AbstractMethods into supposedly fixed AbstractMethods, writes them to\n",
    "        `<model_directory>/output.txt`, and then returns them. Depending on what type of value is passed to\n",
    "        `buggy`, the return value of this method changes according to the following:\n",
    "\n",
    "        | `buggy` type           | Return type            |\n",
    "        | :--------------------- | :--------------------- |\n",
    "        | `str` (a file)         | `List[AbstractMethod]` |\n",
    "        | `AbstractMethod`       | `AbstractMethod`       |\n",
    "        | `List[AbstractMethod]` | `List[AbstractMethod]` |\n",
    "\n",
    "        Optional args:\n",
    "        - `modelFile`: A `.pt` file which is used for translation instead of the default `model_final.pt`\n",
    "        - `applyEditOperations`: When set to True, the model output is interpreted as EditOperations -- a\n",
    "          postprocessing stage occurs where the outputted EditOperations are applied to the inputted\n",
    "          AbstractMethods. When set to False, the raw output is interpreted as AbstractMethods and returned\n",
    "          without a postprocessing stage. If the model was trained with EditOperations, `applyEditOperations`\n",
    "          should be True; if the model was trained with just AbstractMethods as in for the control group,\n",
    "          then this should be False. Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        # determine which model file to use, and raise an error if it doesn't exist\n",
    "        if modelFile is None:\n",
    "            modelFile = self.__FINAL_MODEL_PATH\n",
    "        if not os.path.isfile(modelFile):\n",
    "            raise FileNotFoundError(\"Hephaestus: model not found -- {}\".format(modelFile))\n",
    "        \n",
    "        # write the AbstractMethods to a file if they were given directly\n",
    "        buggyFile = None\n",
    "        if type(buggy) in (AbstractMethod, list):\n",
    "            buggyFile = os.path.join(self.__MODEL_DIR, \"input.txt\")\n",
    "            writeAbstractMethodsToFile(buggyFile, buggy if type(buggy) is list else [buggy])\n",
    "        else:\n",
    "            buggyFile = buggy\n",
    "        \n",
    "        # translate the buggy methods\n",
    "        command = [\"onmt_translate -model '{}' -src '{}' -output '{}'\".format(modelFile, buggyFile, self.__OUTPUT_PATH)]\n",
    "        if getYamlParameter(self.__CONFIG_PATH, \"world_size\") is not None: # if GPU should be used\n",
    "            command += [\"-gpu\", \"0\"]\n",
    "        runCommand(command)\n",
    "\n",
    "        # get all inputted AbstractMethods\n",
    "        inputMethods = []\n",
    "        if type(buggy) in (AbstractMethod, list):\n",
    "            inputMethods = buggy if type(buggy) is list else [buggy]\n",
    "        else:\n",
    "            with open(buggyFile, \"r\") as inputFile:\n",
    "                inputMethods = [AbstractMethod(line.strip()) for line in inputFile.readlines()]\n",
    "\n",
    "        # get all lines of output\n",
    "        outputLines = []\n",
    "        with open(self.__OUTPUT_PATH, \"r\") as outputFile:\n",
    "            outputLines = [line.strip() for line in outputFile.readlines()]\n",
    "        \n",
    "        # iterate through input and output to determine the fixed AbstractMethods\n",
    "        fixedMethods = []\n",
    "        for inputMethod, outputLine in zip(inputMethods, outputLines):\n",
    "            \n",
    "            if applyEditOperations:\n",
    "                # TODO: extract edit operations and apply them to the inputMethod\n",
    "                # inputMethod.applyEditOperations(...)\n",
    "                # fixedMethods.append(inputMethod)\n",
    "                raise RuntimeError(\"HephaestusModel: postprocessing is not supported yet\")\n",
    "\n",
    "            else:\n",
    "                # no postprocessing, just convert the raw output line directly to an AbstractMethod\n",
    "                fixedMethods.append(AbstractMethod(outputLine))\n",
    "\n",
    "        # return the fixed methods\n",
    "        return fixedMethods if type(buggy) is list else fixedMethods[0]\n",
    "\n",
    "    def __writeConfigFile(self, *args, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Creates the config file.\n",
    "        \"\"\"\n",
    "        \n",
    "        lines = [\n",
    "            \"# AUTOGENERATED\",\n",
    "            \"\",\n",
    "            \"# Samples will be writted to here\",\n",
    "            \"save_data: {}\".format(self.__SAVE_DATA_PATH.replace(os.path.sep, \"/\")),\n",
    "            \"\",\n",
    "            \"# Vocabs will be written to these files\",\n",
    "            \"src_vocab: {}\".format(self.__SOURCE_VOCAB_PATH.replace(os.path.sep, \"/\")),\n",
    "            \"tgt_vocab: {}\".format(self.__TARGET_VOCAB_PATH.replace(os.path.sep, \"/\")),\n",
    "            \"\",\n",
    "            \"# Allow overwriting existing files in the directory\",\n",
    "            \"overwrite: True\",\n",
    "            \"\",\n",
    "            \"# Data corpus\",\n",
    "            \"data:\",\n",
    "            \"    corpus_1:\",\n",
    "            \"        path_src: {}\".format(args[0].replace(os.path.sep, \"/\")),\n",
    "            \"        path_tgt: {}\".format(args[1].replace(os.path.sep, \"/\")),\n",
    "            \"        transforms: []\",\n",
    "            \"        weight: 1\",\n",
    "            \"    valid:\",\n",
    "            \"        path_src: {}\".format(args[2].replace(os.path.sep, \"/\")),\n",
    "            \"        path_tgt: {}\".format(args[3].replace(os.path.sep, \"/\")),\n",
    "            \"        transforms: []\",\n",
    "            \"\",\n",
    "            \"# Checkpoints will be saved here\",\n",
    "            \"save_model: {}\".format(self.__SAVE_MODEL_PATH.replace(os.path.sep, \"/\")),\n",
    "            \"save_checkpoint_steps: {}\".format(kwargs[\"saveCheckpointSteps\"]),\n",
    "            \"train_steps: {}\".format(kwargs[\"trainSteps\"]),\n",
    "            \"valid_steps: {}\".format(kwargs[\"validSteps\"]),\n",
    "            \"\"\n",
    "        ]\n",
    "\n",
    "        numGPUs = kwargs[\"numGPUs\"]\n",
    "        if numGPUs > 0:\n",
    "            lines += [\n",
    "                \"# Train using {} GPU{}\".format(numGPUs, \"s\" if numGPUs > 1 else \"\"),\n",
    "                \"world_size: {}\".format(numGPUs),\n",
    "                \"gpu_ranks:\",\n",
    "                *[\"- {}\".format(i) for i in range(numGPUs)]\n",
    "            ]\n",
    "        else:\n",
    "            lines += [\"# Train using the CPU, so no world_size provided\"]\n",
    "\n",
    "        with open(self.__CONFIG_PATH, \"w\") as file:\n",
    "            file.write(\"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"HephaestusModel.train\" class=\"doc_header\"><code>HephaestusModel.train</code><a href=\"__main__.py#L31\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>HephaestusModel.train</code>(**`trainSource`**:`str`, **`trainTarget`**:`str`, **`validSource`**:`str`, **`validTarget`**:`str`, **`vocabSamples`**:`int`=*`10000`*, **`numGPUs`**:`int`=*`1`*, **`trainSteps`**:`int`=*`1000`*, **`validSteps`**:`int`=*`None`*, **`saveCheckpointSteps`**:`int`=*`None`*)\n",
       "\n",
       "Trains the model with the given parameters. Files containing AbstractMethods should have one per line with\n",
       "tokens separated by spaces. 'source' files must contain AbstractMethods. 'target' files may contain\n",
       "AbstractMethods or EditOperations.\n",
       "\n",
       "As the training progesses, checkpoint model files are created which follow the format `model_step_#.pt`, where\n",
       "`#` corresponds to the training step number. Once training is complete, the finalized model is outputted to\n",
       "`model_final.pt`.\n",
       "\n",
       "Required args:\n",
       "- `trainSource`: File name contatining training source data, must be buggy AbstractMethods.\n",
       "- `trainTarget`: File name contatining training target data, can be fixed AbstractMethods or EditOperations\n",
       "  describing buggy -> fixed.\n",
       "- `validSource`: File name contatining validation source data, must be buggy AbstractMethods.\n",
       "- `validTarget`: File name contatining validation target data, must be the same type of data provided in\n",
       "  `trainTarget`.\n",
       "\n",
       "Optional args:\n",
       "- `vocabSamples`: Number of transformed samples per corpus to use when building vocabulary. Defaults to 10000.\n",
       "- `numGPUs`: Number of GPUs to use concurrently during training. If set to 0, then the CPU is used. Defaults to\n",
       "  1.\n",
       "- `trainSteps`: Number of training steps to go through. Defaults to 1000.\n",
       "- `validSteps`: Number of training steps after which each validation occurs; e.g. if `trainSteps` is 1000 and\n",
       "  `validSteps` is 500, then validation will occur after training steps 500 and 1000. Defaults to\n",
       "  `trainSteps` / 2.\n",
       "- `saveCheckpointSteps`: Number of training steps after which model checkpoints are saved; e.g. if `trainSteps`\n",
       "  is 1000 and 'saveCheckpointSteps' is 500, then the model will be saved in after training steps 5000 and 1000.\n",
       "  Defaults to `trainSteps` / 2."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(HephaestusModel.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"HephaestusModel.translate\" class=\"doc_header\"><code>HephaestusModel.translate</code><a href=\"__main__.py#L113\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>HephaestusModel.translate</code>(**`buggy`**:`Union`\\[`str`, [`AbstractMethod`](/hephaestus/AbstractMethod.html), `List`\\[[`AbstractMethod`](/hephaestus/AbstractMethod.html)\\]\\], **`modelFile`**:`str`=*`None`*, **`applyEditOperations`**:`bool`=*`True`*)\n",
       "\n",
       "Translates the given `buggy` AbstractMethods into supposedly fixed AbstractMethods, writes them to\n",
       "`<model_directory>/output.txt`, and then returns them. Depending on what type of value is passed to\n",
       "`buggy`, the return value of this method changes according to the following:\n",
       "\n",
       "| `buggy` type           | Return type            |\n",
       "| :--------------------- | :--------------------- |\n",
       "| `str` (a file)         | `List[AbstractMethod]` |\n",
       "| [`AbstractMethod`](/hephaestus/AbstractMethod.html)       | [`AbstractMethod`](/hephaestus/AbstractMethod.html)       |\n",
       "| `List[AbstractMethod]` | `List[AbstractMethod]` |\n",
       "\n",
       "Optional args:\n",
       "- `modelFile`: A `.pt` file which is used for translation instead of the default `model_final.pt`\n",
       "- `applyEditOperations`: When set to True, the model output is interpreted as EditOperations -- a\n",
       "  postprocessing stage occurs where the outputted EditOperations are applied to the inputted\n",
       "  AbstractMethods. When set to False, the raw output is interpreted as AbstractMethods and returned\n",
       "  without a postprocessing stage. If the model was trained with EditOperations, `applyEditOperations`\n",
       "  should be True; if the model was trained with just AbstractMethods as in for the control group,\n",
       "  then this should be False. Defaults to True."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(HephaestusModel.translate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HephaestusModel(\"test_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-04-09 08:33:22,022 INFO] Counter vocab from 10000 samples.\n",
      "[2021-04-09 08:33:22,022 INFO] Build vocab on 10000 transformed examples/corpus.\n",
      "[2021-04-09 08:33:22,044 INFO] corpus_1's transforms: TransformPipe()\n",
      "[2021-04-09 08:33:22,055 INFO] Loading ParallelCorpus(../data/small/train_buggy.txt, ../data/small/train_fixed.txt, align=None)...\n",
      "[2021-04-09 08:33:22,625 INFO] Counters src:411\n",
      "[2021-04-09 08:33:22,625 INFO] Counters tgt:403\n",
      "[2021-04-09 08:33:22,629 WARNING] path test_model/save_data.vocab.src exists, may overwrite...\n",
      "[2021-04-09 08:33:22,646 WARNING] path test_model/save_data.vocab.tgt exists, may overwrite...\n",
      "[2021-04-09 08:33:23,707 INFO] Parsed 2 corpora from -data.\n",
      "[2021-04-09 08:33:23,714 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n",
      "[2021-04-09 08:33:23,714 INFO] Loading vocab from text file...\n",
      "[2021-04-09 08:33:23,714 INFO] Loading src vocabulary from test_model/save_data.vocab.src\n",
      "[2021-04-09 08:33:23,732 INFO] Loaded src vocab has 411 tokens.\n",
      "[2021-04-09 08:33:23,732 INFO] Loading tgt vocabulary from test_model/save_data.vocab.tgt\n",
      "[2021-04-09 08:33:23,743 INFO] Loaded tgt vocab has 403 tokens.\n",
      "[2021-04-09 08:33:23,743 INFO] Building fields with vocab in counters...\n",
      "[2021-04-09 08:33:23,744 INFO]  * tgt vocab size: 407.\n",
      "[2021-04-09 08:33:23,745 INFO]  * src vocab size: 413.\n",
      "[2021-04-09 08:33:23,745 INFO]  * src vocab size = 413\n",
      "[2021-04-09 08:33:23,745 INFO]  * tgt vocab size = 407\n",
      "[2021-04-09 08:33:23,745 INFO] Building model...\n",
      "[2021-04-09 08:33:23,956 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(413, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(500, 500, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(407, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(1000, 500)\n",
      "        (1): LSTMCell(500, 500)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (linear_out): Linear(in_features=1000, out_features=500, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=500, out_features=407, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "[2021-04-09 08:33:23,956 INFO] encoder: 4214500\n",
      "[2021-04-09 08:33:23,956 INFO] decoder: 6165407\n",
      "[2021-04-09 08:33:23,956 INFO] * number of parameters: 10379907\n",
      "[2021-04-09 08:33:23,961 INFO] Starting training on CPU, could be very slow\n",
      "[2021-04-09 08:33:23,961 INFO] Start training loop and validate every 100 steps...\n",
      "[2021-04-09 08:33:23,961 INFO] corpus_1's transforms: TransformPipe()\n",
      "[2021-04-09 08:33:23,971 INFO] Loading ParallelCorpus(../data/small/train_buggy.txt, ../data/small/train_fixed.txt, align=None)...\n",
      "[2021-04-09 08:36:18,870 INFO] Step 50/  500; acc:  11.31; ppl: 1907.46; xent: 7.55; lr: 1.00000; 566/539 tok/s;    175 sec\n",
      "[2021-04-09 08:39:03,209 INFO] Step 100/  500; acc:  21.93; ppl: 76.95; xent: 4.34; lr: 1.00000; 628/588 tok/s;    339 sec\n",
      "[2021-04-09 08:39:03,210 INFO] valid's transforms: TransformPipe()\n",
      "[2021-04-09 08:39:03,215 INFO] Loading ParallelCorpus(../data/small/valid_buggy.txt, ../data/small/valid_fixed.txt, align=None)...\n",
      "[2021-04-09 08:40:51,307 INFO] Validation perplexity: 33.7895\n",
      "[2021-04-09 08:40:51,307 INFO] Validation accuracy: 23.6846\n",
      "[2021-04-09 08:43:34,191 INFO] Step 150/  500; acc:  35.69; ppl: 20.05; xent: 3.00; lr: 1.00000; 374/354 tok/s;    610 sec\n",
      "[2021-04-09 08:46:13,830 INFO] Step 200/  500; acc:  44.62; ppl: 11.19; xent: 2.42; lr: 1.00000; 627/597 tok/s;    770 sec\n",
      "[2021-04-09 08:46:13,835 INFO] Loading ParallelCorpus(../data/small/valid_buggy.txt, ../data/small/valid_fixed.txt, align=None)...\n",
      "[2021-04-09 08:47:59,314 INFO] Validation perplexity: 7.99917\n",
      "[2021-04-09 08:47:59,314 INFO] Validation accuracy: 49.9935\n",
      "[2021-04-09 08:50:38,974 INFO] Step 250/  500; acc:  48.57; ppl:  9.14; xent: 2.21; lr: 1.00000; 383/361 tok/s;   1035 sec\n",
      "[2021-04-09 08:50:38,979 INFO] Saving checkpoint test_model/model_step_250.pt\n",
      "[2021-04-09 08:53:24,451 INFO] Step 300/  500; acc:  52.27; ppl:  7.33; xent: 1.99; lr: 1.00000; 612/583 tok/s;   1200 sec\n",
      "[2021-04-09 08:53:24,458 INFO] Loading ParallelCorpus(../data/small/valid_buggy.txt, ../data/small/valid_fixed.txt, align=None)...\n",
      "[2021-04-09 08:55:12,916 INFO] Validation perplexity: 6.6979\n",
      "[2021-04-09 08:55:12,916 INFO] Validation accuracy: 50.5736\n",
      "[2021-04-09 08:58:03,530 INFO] Step 350/  500; acc:  54.84; ppl:  6.41; xent: 1.86; lr: 1.00000; 373/349 tok/s;   1480 sec\n",
      "[2021-04-09 09:01:00,685 INFO] Step 400/  500; acc:  56.51; ppl:  5.91; xent: 1.78; lr: 1.00000; 570/541 tok/s;   1657 sec\n",
      "[2021-04-09 09:01:00,691 INFO] Loading ParallelCorpus(../data/small/valid_buggy.txt, ../data/small/valid_fixed.txt, align=None)...\n",
      "[2021-04-09 09:02:51,454 INFO] Validation perplexity: 4.954\n",
      "[2021-04-09 09:02:51,454 INFO] Validation accuracy: 61.0846\n",
      "[2021-04-09 09:05:54,403 INFO] Step 450/  500; acc:  58.99; ppl:  5.24; xent: 1.66; lr: 1.00000; 346/325 tok/s;   1950 sec\n",
      "[2021-04-09 09:08:57,865 INFO] Step 500/  500; acc:  59.61; ppl:  5.06; xent: 1.62; lr: 1.00000; 556/524 tok/s;   2134 sec\n",
      "[2021-04-09 09:08:57,870 INFO] Loading ParallelCorpus(../data/small/valid_buggy.txt, ../data/small/valid_fixed.txt, align=None)...\n",
      "[2021-04-09 09:10:47,276 INFO] Validation perplexity: 4.40575\n",
      "[2021-04-09 09:10:47,276 INFO] Validation accuracy: 61.2692\n",
      "[2021-04-09 09:10:47,283 INFO] Saving checkpoint test_model/model_step_500.pt\n"
     ]
    }
   ],
   "source": [
    "# collapse_output\n",
    "model.train(\n",
    "    \"../data/small/train_buggy.txt\",\n",
    "    \"../data/small/train_fixed.txt\",\n",
    "    \"../data/small/valid_buggy.txt\",\n",
    "    \"../data/small/valid_fixed.txt\",\n",
    "    numGPUs = 0,\n",
    "    trainSteps = 500,\n",
    "    validSteps = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "private TYPE_1 getType ( TYPE_2 VAR_1 ) { TYPE_3 VAR_2 = new TYPE_3 ( STRING_1 ) ; return new TYPE_1 ( VAR_2 , VAR_2 ) ; }"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buggyMethod = readAbstractMethodsFromFile(\"../data/small/test_buggy.txt\")[0]\n",
    "buggyMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-04-09 09:20:31,950 INFO] Translating shard 0.\n",
      "[2021-04-09 09:20:32,076 INFO] PRED AVG SCORE: -0.5723, PRED PPL: 1.7724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "private void METHOD_1 ( java.lang.String VAR_1 ) { return VAR_1 ; }"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixedMethod = model.translate(buggyMethod, modelFile = \"test_model/model_step_250.pt\", applyEditOperations = False)\n",
    "fixedMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "private TYPE_1 getType ( TYPE_2 VAR_1 ) { TYPE_3 VAR_2 = new TYPE_3 ( STRING_1 ) ; return new TYPE_1 ( VAR_2 , VAR_2 , this , VAR_1 ) ; }"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actualFixedMethod = readAbstractMethodsFromFile(\"../data/small/test_fixed.txt\")[0]\n",
    "actualFixedMethod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "643e2b53eabc93377e95b07774b60b49bf379f8a09a9dbf1d193d7cd19dee0e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
